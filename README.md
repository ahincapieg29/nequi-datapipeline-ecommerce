# üõí eCommerce Data Pipeline

Dise√±o e implementaci√≥n de un pipeline moderno de datos en AWS para capturar, procesar y analizar eventos de comportamiento de usuarios en una tienda de comercio electr√≥nico.

## üß© Paso 1: Alcance del Proyecto y Captura de Datos

### üéØ Objetivo

Dise√±ar una arquitectura escalable de datos para una empresa de comercio electr√≥nico, enfocada en capturar, procesar y estructurar eventos de comportamiento de usuarios (como vistas de producto, adiciones al carrito y compras). El objetivo es habilitar flujos de valor anal√≠tico para **nutrir de datos a toda la compa√±√≠a**, soportando operaciones, Business Intelligence (BI) y Ciencia de Datos (DS).

Este ejercicio simula, a partir de un conjunto de datos de ejemplo descargado desde Kaggle, el dise√±o e implementaci√≥n de un pipeline de datos moderno: desde la ingesta hasta el modelado anal√≠tico, aplicando buenas pr√°cticas de calidad, gobierno y rendimiento sobre una arquitectura en AWS.

---

### üìÅ Dataset

- **Nombre:** eCommerce behavior data from multi category store  
- **Fuente:** [Kaggle - eCommerce behavior data](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store) 
- **Archivo utilizado:** `2020-Apr.csv.gz`  
- **Tama√±o:** 66.589.268 registros  
- **Formato:** CSV comprimido (.gz)  
- **Frecuencia:** Abril 2020  

**Campos principales:**

- `event_time`: Fecha y hora del evento  
- `event_type`: Tipo de evento (`view`, `cart`, `remove_from_cart`, `purchase`)  
- `product_id`: ID del producto  
- `category_id`: ID de la categor√≠a  
- `category_code`: Categor√≠a espec√≠fica del producto  
- `brand`: Marca del producto  
- `price`: Precio del producto  
- `user_id`: ID an√≥nimo del usuario  
- `user_session`: ID de la sesi√≥n de navegaci√≥n  

üìå *Limitaciones conocidas:*  
El dataset solo representa un mes de eventos, y las sesiones de usuario son an√≥nimas. No incluye datos de usuarios autenticados ni eventos offline.

---

### üì¶ Captura de Datos

El archivo fue descargado desde Kaggle, descomprimido y ubicado en la ruta del proyecto:

```
/datos/raw/2020-Abr.csv.gz
```

Este ser√° utilizado como **fuente principal de datos** para construir el pipeline.

---

### üîç Casos de Uso Esperados

Este proyecto representa una soluci√≥n de datos para un **eCommerce** que busca consolidar una plataforma anal√≠tica empresarial capaz de alimentar diferentes √°reas:

#### üß† Para la organizaci√≥n

- Plataforma de datos unificada y gobernada  
- Integraci√≥n entre equipos de marketing, operaciones, BI y ciencia de datos  
- Toma de decisiones basada en datos reales del comportamiento de usuarios  

#### üß™ Business Intelligence (BI)

- Embudo de conversi√≥n: `view ‚Üí cart ‚Üí purchase`  
- Ranking de productos con mayor visualizaci√≥n vs. menor conversi√≥n  
- An√°lisis de sesiones por usuario y duraci√≥n promedio  
- Top categor√≠as y marcas por volumen de ventas  
- Evoluci√≥n de eventos por hora, d√≠a y semana  
- Comparativo de precios promedio por categor√≠a y marca  
- Identificaci√≥n de usuarios m√°s activos y recurrentes  

#### üî¨ Ciencia de Datos (Data Science)

- Segmentaci√≥n de usuarios basada en comportamiento  
- Predicci√≥n de probabilidad de compra  
- Detecci√≥n de anomal√≠as en precios o eventos  
- Clustering de productos por interacci√≥n y conversi√≥n  
- Modelos de propensi√≥n al abandono de carrito  
- Sistemas de recomendaci√≥n personalizados  

---

## üß© Paso 2: Exploraci√≥n y Evaluaci√≥n de Datos (EDA)

Para analizar un dataset de m√°s de **66 millones de registros**, se utiliz√≥ **PySpark** como motor de procesamiento distribuido. Gracias a su escalabilidad, se pudieron ejecutar transformaciones complejas y validaciones sin saturar el entorno de desarrollo.

Se tom√≥ una muestra aleatoria de aproximadamente **1.5 millones de registros** (~2.3% del total), lo que permiti√≥ realizar un **an√°lisis exploratorio eficiente** preservando la diversidad de tipos de eventos, productos y usuarios.

El an√°lisis exploratorio se realiz√≥ utilizando PySpark y se document√≥ en el notebook 

```
/notebooks/eda.ipynb.
```

---

### üîç Exploraci√≥n: Calidad de los Datos

#### üìå A. Valores Nulos Detectados

| Columna          | Valores Nulos |
|------------------|----------------|
| `brand`          | 205,961        |
| `category_code`  | 155,385        |
| `user_session`   | 2              |
| Resto de columnas| 0              |

#### üìå B. Registros Duplicados

- Total registros: **1,531,767**
- Registros √∫nicos: **1,531,619**
- **Duplicados detectados:** 148

#### üìå C. Valores √önicos por Columna

- `user_id`: 892,389
- `product_id`: 124,922
- `brand`: 3,661
- `category_code`: 139
- `event_time`: 1,085,863
- `user_session`: 1,281,641

#### üìå D. Tipos de Evento

| Tipo de evento | Registros |
|----------------|-----------|
| `view`         | 1,434,849 |
| `cart`         | 74,880    |
| `purchase`     | 22,038    |

‚Üí Representaci√≥n t√≠pica del embudo de conversi√≥n eCommerce: vistas > carritos > compras.

#### üìå E. Estad√≠sticas del Precio

| M√©trica   | Valor       |
|-----------|-------------|
| Count     | 1,531,767   |
| Media     | $273.11     |
| Desviaci√≥n est√°ndar | $356.13 |
| M√≠nimo    | $0.00       |
| M√°ximo    | $2,574.07   |

---

### üßº Sugerencias para la Limpieza de Datos

A partir de los hallazgos previos, se proponen las siguientes estrategias de limpieza para mejorar la calidad de los datos antes del modelado:

1. **Conversi√≥n de tipos**
   - Convertir `event_time` a `timestamp` con zona horaria UTC.
   - Tipificar `event_type` como variable categ√≥rica controlada (`view`, `cart`, `purchase`).

2. **Eliminaci√≥n de duplicados**
   - Remover registros completamente duplicados (id√©nticos en todas las columnas).

3. **Tratamiento de valores nulos**
   - Imputar `brand` y `category_code` con `"unknown"` cuando el porcentaje de nulos por categor√≠a sea bajo (<5%).
   - Omitir registros con `user_session` nulo (2 casos identificados).

4. **Filtrado de precios inv√°lidos**
   - Excluir registros con precio igual a 0 o negativo, ya que no representan comportamiento v√°lido.

5. **Verificaci√≥n de relaciones l√≥gicas**
   - Validar consistencia entre `user_id`, `user_session` y secuencia de `event_type`.
   - Confirmar flujos completos del embudo de conversi√≥n en sesiones (`view ‚Üí cart ‚Üí purchase`).

---

### üß™ Justificaci√≥n del Muestreo y Uso de PySpark

- üß† **Muestreo controlado (~1.5M filas)**: permite acelerar el desarrollo local sin sacrificar representatividad estad√≠stica.
- üî• **PySpark**: motor de procesamiento distribuido ideal para trabajar con datasets a gran escala como el original (66M+ registros), habilitando limpieza, transformaci√≥n y an√°lisis eficiente sobre AWS Glue u otros entornos.

---

## üß© Paso 3: Definici√≥n del Modelo de Datos y Arquitectura

### üß≠ Contexto

En un entorno de eCommerce moderno, los usuarios interact√∫an con una aplicaci√≥n m√≥vil generando millones de eventos mensuales (vistas de producto, adiciones al carrito, compras, etc.). Estos eventos son almacenados inicialmente en una **base de datos transaccional (OLTP)** como Aurora PostgreSQL, optimizada para escritura y consistencia. A partir de all√≠, se construye un pipeline para transformar y preparar los datos para uso anal√≠tico, dashboards de BI y ciencia de datos.

---

### üóÇÔ∏è Modelo de Datos Conceptual (OLTP)

#### Dise√±o inicial en Aurora PostgreSQL

La base de datos transaccional se dise√±√≥ utilizando un modelo **normalizado** con integridad referencial, ideal para registrar actividad desde la app m√≥vil en tiempo real.

**Entidades principales (PK y FK):**

- **USERS**  
  - `user_id` (Primary Key)  
  - `name`  
  - `email`  
  - `created_at`  

- **PRODUCTS**  
  - `product_id` (Primary Key)  
  - `name`  
  - `brand_id` (Foreign Key ‚Üí BRANDS.brand_id)  
  - `category_id` (Foreign Key ‚Üí CATEGORIES.category_id)  
  - `price`  
  - `stock`  

- **CATEGORIES**  
  - `category_id` (Primary Key)  
  - `category_name`  
  - `parent_id` (Foreign Key ‚Üí CATEGORIES.category_id)  

- **BRANDS**  
  - `brand_id` (Primary Key)  
  - `brand_name`  

- **SESSIONS**  
  - `session_id` (Primary Key)  
  - `user_id` (Foreign Key ‚Üí USERS.user_id)  
  - `device_type`  
  - `channel`  
  - `started_at`  

- **EVENTS**  
  - `event_id` (Primary Key)  
  - `session_id` (Foreign Key ‚Üí SESSIONS.session_id)  
  - `product_id` (Foreign Key ‚Üí PRODUCTS.product_id)  
  - `event_type` (ENUM: view, cart, purchase)  
  - `event_time`  
  - `price`  

**Ventajas del modelo OLTP:**

- Alta normalizaci√≥n garantiza consistencia y evita duplicaci√≥n  
- Relaciones referenciales para trazabilidad completa: usuario ‚Üí sesi√≥n ‚Üí evento  
- Optimizado para escritura intensiva  
- Preparado para replicaci√≥n CDC mediante AWS DMS hacia S3  

---

### üß± Modelo Anal√≠tico (Data Lake)

Una vez en S3, se aplica un proceso ETL para construir un modelo de datos orientado a an√°lisis.

#### Modelo en estrella optimizado para Athena / Redshift:

- **Tabla de hechos:** `fact_user_events`  
  - `event_id`, `event_time`, `event_type`  
  - `user_id`, `product_id`, `price`, `session_id`  
  - `category_name`, `brand` (denormalizados)  
  - `device_type`, `source_channel`, `day_of_week`, `hour_of_day`  

- **Dimensiones:**  
  - `dim_users`: Perfil de usuario  
  - `dim_time`: Calendario  

**Capas del Data Lake en S3:**

- `raw/`: ingesti√≥n bruta desde DMS  
- `clean/`: datos validados, transformados  
- `model/`: modelo en estrella en formato Parquet  

---

### ‚öôÔ∏è Herramientas y Tecnolog√≠as Elegidas

| Componente                  | Tecnolog√≠a                         | Motivo de elecci√≥n                                                   |
|----------------------------|-------------------------------------|----------------------------------------------------------------------|
| Base de datos OLTP         | Aurora PostgreSQL                   | Escalable, transaccional, ideal para app m√≥vil                      |
| Replicaci√≥n continua       | AWS DMS (CDC)                       | Sincroniza datos sin afectar OLTP                                   |
| Almacenamiento             | Amazon S3                           | Econ√≥mico, escalable, nativo para Data Lake                         |
| Transformaci√≥n             | AWS Glue + PySpark                  | Procesamiento distribuido sobre alto volumen                        |
| Organizaci√≥n de datos      | Data Lake por capas (raw-clean-model)| Mejora trazabilidad, modularidad y control                          |
| Consulta anal√≠tica         | Athena                              | SQL serverless, bajo costo, ideal para exploraci√≥n y BI             |
| Visualizaci√≥n              | Amazon QuickSight, Power BI         | Integraci√≥n directa con Athena y Redshift                           |
| Formato de almacenamiento  | Parquet                             | Columnar, comprimido, altamente eficiente en an√°lisis               |

- AWS Glue fue elegido sobre Lambda + Step Functions porque el volumen de datos (66M+) y las transformaciones requeridas (join, filtrado, particionado) se benefician del procesamiento distribuido con PySpark.
- Aurora PostgreSQL permite escalabilidad transaccional con r√©plicas, ideal para integraci√≥n con CDC (Change Data Capture) usando AWS DMS.
- S3 es el almacenamiento √≥ptimo para un Data Lake escalable, y permite separaci√≥n por capas (`raw`, `clean`, `model`) con esquemas evolutivos.

---

### üîÅ Frecuencia de Actualizaci√≥n Recomendada

**Propuesta:** Actualizaci√≥n cada **1 hora** mediante **microlotes** para la tabla de hechos `fact_user_events`, y cargas **diarias** para dimensiones maestras (`dim_users`, `dim_products`).

**Justificaci√≥n:**

#### Para Business Intelligence (BI):
- Una actualizaci√≥n **cada hora** es suficiente para:
  - Monitorear comportamiento de usuarios en tiempo operativo
  - Medir rendimiento de campa√±as activas sin necesidad de real-time
  - Mantener dashboards √°giles con bajo costo computacional
  - Compatible con Power BI, QuickSight y Athena (consulta sobre particiones por fecha).

#### Para Ciencia de Datos (DS):
- Cargas **diarias** permiten:
  - Entrenamiento eficiente de modelos predictivos y an√°lisis exploratorio
  - Preparaci√≥n de features hist√≥ricas para clustering, scoring y segmentaci√≥n
  - Menor carga operativa y m√°s estabilidad en pipelines de entrenamiento

#### Capacidad t√©cnica:
- **AWS DMS** permite replicaci√≥n continua desde Aurora PostgreSQL hacia S3 (`raw/`).
- **AWS Glue** se puede ejecutar por cron cada hora para transformar solo los nuevos datos del d√≠a (`PROCESS_DATE=HOY`).
- El particionado por `event_date` permite cargas y consultas optimizadas en Athena y Redshift Spectrum.

---

### ‚úÖ Conclusi√≥n

Esta arquitectura permite:

- Separar las cargas OLTP de las anal√≠ticas, preservando rendimiento  
- Ingestar y transformar datos a gran escala sin impacto en producci√≥n  
- Ejecutar dashboards y modelos de an√°lisis con datos frescos y organizados  
- Evolucionar f√°cilmente hacia Redshift o Snowflake si la carga lo requiere  

La soluci√≥n cumple con las mejores pr√°cticas de AWS para arquitectura anal√≠tica moderna, aplicando herramientas serverless, formatos columnarizados, y un modelo escalable sin dependencias innecesarias.

---

## üß© Paso 4: Construcci√≥n del ETL

Este paso implementa una **pipeline ETL modular y escalable** que procesa eventos de comportamiento de usuarios desde Aurora PostgreSQL (v√≠a AWS DMS) hacia un modelo anal√≠tico en S3 en formato Parquet. Se ejecuta cada hora para tablas de hechos y diariamente para dimensiones maestras.

---

## ‚öôÔ∏è Arquitectura del Proceso

```
Aurora PostgreSQL (OLTP)
   ‚Üì (CDC via AWS DMS)
S3 Bucket (raw/)
   ‚Üì (PySpark en AWS Glue)
S3 (clean/, model/)
   ‚Üì
Athena / Power BI / QuickSight
```

---

## üìÅ Estructura del Proyecto ETL

```bash
/etl/
‚îú‚îÄ‚îÄ extract/
‚îÇ   ‚îî‚îÄ‚îÄ extract_from_s3.py
‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îú‚îÄ‚îÄ clean_and_transform_events.py
‚îÇ   ‚îî‚îÄ‚îÄ transform_dimensions.py
‚îú‚îÄ‚îÄ load/
‚îÇ   ‚îî‚îÄ‚îÄ load_to_model.py
‚îú‚îÄ‚îÄ quality/
‚îÇ   ‚îî‚îÄ‚îÄ quality_checks.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ unit_tests_etl.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ spark_session.py
‚îú‚îÄ‚îÄ run_etl.py                          # Orquestador del proceso completo
‚îú‚îÄ‚îÄ data_dictionary.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üß† Orquestador Principal

**Archivo:** `run_etl.py`

```python
# run_etl.py

"""
Orquesta la ejecuci√≥n completa del pipeline ETL:
1. Extrae datos de eventos desde S3/raw
2. Aplica limpieza y transformaci√≥n
3. Carga resultados en S3/model particionado
"""

from extract.extract_from_s3 import extract_events
from transform.clean_and_transform_events import clean_transform
from load.load_to_model import load_events
from utils.spark_session import get_spark_session
import logging

# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    try:
        logger.info("üöÄ Iniciando pipeline ETL completo...")

        # Inicializar sesi√≥n de Spark
        spark = get_spark_session("ETL-Runner")

        # Paso 1: Extracci√≥n
        df_raw = extract_events()

        # Paso 2: Transformaci√≥n
        df_transformed = clean_transform(df_raw)

        # Paso 3: Carga
        load_events(df_transformed)

        logger.info("‚úÖ ETL ejecutado exitosamente.")

    except Exception as e:
        logger.error(f"‚ùå Error en la ejecuci√≥n del ETL: {str(e)}")
        raise
```

---

## 1Ô∏è‚É£ Extracci√≥n de Datos

**Archivo:** `extract/extract_from_s3.py`

```python
# extract_from_s3.py

"""
Carga los datos de eventos desde la capa raw en S3 y filtra los del d√≠a actual.
"""

from utils.spark_session import get_spark_session
from pyspark.sql.functions import current_date
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_events():
    spark = get_spark_session("ExtractEvents")

    logger.info("üì¶ Leyendo datos desde S3: raw/events")
    df = spark.read.parquet("s3://ecommerce-lake/raw/events/")

    # Filtra solo los eventos del d√≠a actual (etl horaria)
    df_today = df.filter(df.event_date == current_date())

    logger.info(f"‚úÖ Registros le√≠dos para hoy: {df_today.count()}")
    return df_today
```

---

## 2Ô∏è‚É£ Transformaci√≥n y Limpieza

**Archivo:** `transform/clean_and_transform_events.py`

```python
# clean_and_transform_events.py

"""
Aplica limpieza, enriquecimiento y validaciones de calidad a los datos extra√≠dos.
"""

from pyspark.sql.functions import col, hour, dayofweek
from quality.quality_checks import check_row_counts, check_nulls
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def clean_transform(df):
    logger.info("üßπ Iniciando limpieza y transformaci√≥n...")

    # Elimina duplicados exactos
    df_clean = df.dropDuplicates()

    # Filtra precios inv√°lidos
    df_clean = df_clean.filter(col("price") > 0)

    # Imputaci√≥n de valores nulos
    df_clean = df_clean.fillna({
        "brand": "unknown",
        "category_code": "unknown"
    }).filter(col("user_session").isNotNull())

    # Enriquecimiento de datos
    df_transformed = df_clean \
        .withColumn("hour_of_day", hour("event_time")) \
        .withColumn("day_of_week", dayofweek("event_time"))

    # Controles de calidad
    check_row_counts(df_transformed, min_expected=10000)
    check_nulls(df_transformed, ["event_time", "event_type", "user_id", "product_id"])

    logger.info("‚úÖ Transformaci√≥n completada exitosamente.")
    return df_transformed
```

---

## 3Ô∏è‚É£ Carga de Datos

**Archivo:** `load/load_to_model.py`

```python
# load_to_model.py

"""
Escribe los datos limpios y transformados en formato Parquet particionado por fecha.
"""

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_events(df_transformed):
    output_path = "s3://ecommerce-lake/model/fact_user_events/"

    logger.info(f"üíæ Escribiendo datos a: {output_path}")
    df_transformed.write.mode("overwrite") \
        .partitionBy("event_date") \
        .parquet(output_path)

    logger.info("‚úÖ Carga exitosa en capa model.")
```

---

## üß™ Validaciones de Calidad

**Archivo:** `quality/quality_checks.py`

```python
# quality_checks.py

"""
Funciones para validar integridad de datos:
- Conteo m√≠nimo
- Nulls
- Esquema
"""

from pyspark.sql.functions import col

def check_row_counts(df, min_expected):
    count = df.count()
    assert count >= min_expected, f"‚ùå Fila insuficiente: {count} < {min_expected}"

def check_nulls(df, cols):
    for col_name in cols:
        nulls = df.filter(col(col_name).isNull()).count()
        assert nulls == 0, f"‚ùå Nulls en columna {col_name}: {nulls}"
```

---

## üß™ Pruebas Unitarias

**Archivo:** `tests/unit_tests_etl.py`

```python
# unit_tests_etl.py

"""
Pruebas autom√°ticas para validar las funciones de calidad de datos.
"""

import unittest
from quality import quality_checks
from pyspark.sql import SparkSession

class TestETLQuality(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.spark = SparkSession.builder.master("local[*]").appName("UnitTest").getOrCreate()
        cls.df = cls.spark.createDataFrame([(1, "view")], ["user_id", "event_type"])

    def test_row_count_pass(self):
        quality_checks.check_row_counts(self.df, 1)

    def test_null_check(self):
        quality_checks.check_nulls(self.df, ["user_id"])

if __name__ == '__main__':
    unittest.main()
```

---

## üîÅ Reproducibilidad y Mantenibilidad

- **Particionado por `event_date`**
- **Logs estructurados** y trazables
- **C√≥digo versionado y testeado**
- **Par√°metros reutilizables**
- Compatible con **AWS Glue, Airflow, Step Functions**

---

## üìò Diccionario de Datos

Ver archivo [`data_dictionary.md`](./data_dictionary.md) para la descripci√≥n completa del modelo `fact_user_events` y sus dimensiones.

---

## üß© Paso 5: Escenarios de Escalabilidad y Arquitectura Alternativa

- **üìà Si los datos crecieran 100x:**  
  Escalar√≠a Glue con Spark m√°s nodos, usaria Redshift Spectrum o EMR para anal√≠tica distribuida. Controlar√≠a particionamiento en S3 por `event_date`.

- **‚è± Si las tuber√≠as se ejecutaran diariamente en una ventana de tiempo espec√≠fica:**  
  Usar√≠a AWS Glue triggers + workflows + monitoreo con CloudWatch y alertas por SNS.

- **üë• Si m√°s de 100 usuarios funcionales accedieran a la BD:**  
  Implementar√≠a Redshift + Amazon SSO + rol de acceso y pol√≠ticas IAM controladas por recurso.

- **‚ö° Si se requiere anal√≠tica en tiempo real:**  
  Cambiar√≠a de arquitectura batch a **Kinesis Data Streams** + **Lambda + Firehose** + **Athena o Redshift Streaming**.
