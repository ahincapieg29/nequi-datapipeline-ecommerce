# ğŸ›’ eCommerce Data Pipeline

DiseÃ±o e implementaciÃ³n de un pipeline moderno de datos en AWS para capturar, procesar y analizar eventos de comportamiento de usuarios en una tienda de comercio electrÃ³nico.

## ğŸ§© Paso 1: Alcance del Proyecto y Captura de Datos

### ğŸ¯ Objetivo

DiseÃ±ar una arquitectura escalable de datos para una empresa de comercio electrÃ³nico, enfocada en capturar, procesar y estructurar eventos de comportamiento de usuarios (como vistas de producto, adiciones al carrito y compras). El objetivo es habilitar flujos de valor analÃ­tico para **nutrir de datos a toda la compaÃ±Ã­a**, soportando operaciones, Business Intelligence (BI) y Ciencia de Datos (DS).

Este ejercicio simula, a partir de un conjunto de datos de ejemplo descargado desde Kaggle, el diseÃ±o e implementaciÃ³n de un pipeline de datos moderno: desde la ingesta hasta el modelado analÃ­tico, aplicando buenas prÃ¡cticas de calidad, gobierno y rendimiento sobre una arquitectura en AWS.

---

### ğŸ“ Dataset

- **Nombre:** eCommerce behavior data from multi category store  
- **Fuente:** Kaggle  
- **Archivo utilizado:** `2020-Apr.csv.gz`  
- **TamaÃ±o:** 66.589.268 registros  
- **Formato:** CSV comprimido (.gz)  
- **Frecuencia:** Abril 2020  

**Campos principales:**

- `event_time`: Fecha y hora del evento  
- `event_type`: Tipo de evento (`view`, `cart`, `remove_from_cart`, `purchase`)  
- `product_id`: ID del producto  
- `category_id`: ID de la categorÃ­a  
- `category_code`: CategorÃ­a especÃ­fica del producto  
- `brand`: Marca del producto  
- `price`: Precio del producto  
- `user_id`: ID anÃ³nimo del usuario  
- `user_session`: ID de la sesiÃ³n de navegaciÃ³n  

ğŸ“Œ *Limitaciones conocidas:*  
El dataset solo representa un mes de eventos, y las sesiones de usuario son anÃ³nimas. No incluye datos de usuarios autenticados ni eventos offline.

---

### ğŸ“¦ Captura de Datos

El archivo fue descargado desde Kaggle, descomprimido y ubicado en la ruta del proyecto:

```
/datos/raw/2020-Abr.csv.gz
```

Este serÃ¡ utilizado como **fuente principal de datos** para construir el pipeline.

---

### ğŸ” Casos de Uso Esperados

Este proyecto representa una soluciÃ³n de datos para un **eCommerce** que busca consolidar una plataforma analÃ­tica empresarial capaz de alimentar diferentes Ã¡reas:

#### ğŸ§  Para la organizaciÃ³n

- Plataforma de datos unificada y gobernada  
- IntegraciÃ³n entre equipos de marketing, operaciones, BI y ciencia de datos  
- Toma de decisiones basada en datos reales del comportamiento de usuarios  

#### ğŸ“Š Business Intelligence (BI)

- Embudo de conversiÃ³n: `view â†’ cart â†’ purchase`  
- Ranking de productos con mayor visualizaciÃ³n vs. menor conversiÃ³n  
- AnÃ¡lisis de sesiones por usuario y duraciÃ³n promedio  
- Top categorÃ­as y marcas por volumen de ventas  
- EvoluciÃ³n de eventos por hora, dÃ­a y semana  
- Comparativo de precios promedio por categorÃ­a y marca  
- IdentificaciÃ³n de usuarios mÃ¡s activos y recurrentes  

#### ğŸ”¬ Ciencia de Datos (Data Science)

- SegmentaciÃ³n de usuarios basada en comportamiento  
- PredicciÃ³n de probabilidad de compra  
- DetecciÃ³n de anomalÃ­as en precios o eventos  
- Clustering de productos por interacciÃ³n y conversiÃ³n  
- Modelos de propensiÃ³n al abandono de carrito  
- Sistemas de recomendaciÃ³n personalizados  

---

## ğŸ“Š Paso 2: ExploraciÃ³n y EvaluaciÃ³n de Datos (EDA)

Se realizÃ³ un anÃ¡lisis exploratorio en el notebook [`notebooks/eda.ipynb`](notebooks/eda.ipynb), donde se identificaron las siguientes observaciones:

### ğŸš¨ Problemas detectados:
- Valores nulos en `brand` y `category_code`
- Algunos precios `0` o negativos
- Eventos `remove_from_cart` muy escasos
- VariaciÃ³n alta en frecuencia de usuarios
- Sesiones con mÃºltiples eventos duplicados

### ğŸ§¼ Estrategia de limpieza:
- Eliminar duplicados exactos
- Imputar `brand` o `category_code` solo si representan < 5%
- Convertir `event_time` a timestamp y generar campos derivados (dÃ­a, hora, etc.)
- Filtrar productos con precio no vÃ¡lido
- ValidaciÃ³n cruzada de relaciones (user-session-event)

---

## ğŸ§© Paso 3: DefiniciÃ³n del Modelo de Datos y Arquitectura

### ğŸ§­ Contexto

En un entorno de eCommerce moderno, los usuarios interactÃºan con una aplicaciÃ³n mÃ³vil generando millones de eventos mensuales (vistas de producto, adiciones al carrito, compras, etc.). Estos eventos son almacenados inicialmente en una **base de datos transaccional (OLTP)** como Aurora PostgreSQL, optimizada para escritura y consistencia. A partir de allÃ­, se construye un pipeline para transformar y preparar los datos para uso analÃ­tico, dashboards de BI y ciencia de datos.

---

### ğŸ—‚ï¸ Modelo de Datos Conceptual (OLTP)

#### DiseÃ±o inicial en Aurora PostgreSQL

La base de datos transaccional se diseÃ±Ã³ utilizando un modelo **normalizado** con integridad referencial, ideal para registrar actividad desde la app mÃ³vil en tiempo real.

**Entidades principales (PK y FK):**

- **USERS**  
  - `user_id` (PK)  
  - `name`  
  - `email`  
  - `created_at`  

- **PRODUCTS**  
  - `product_id` (PK.)  
  - `name`  
  - `brand_id` (FK â†’ BRANDS.brand_id)  
  - `category_id` (FK â†’ CATEGORIES.category_id)  
  - `price`  
  - `stock`  

- **CATEGORIES**  
  - `category_id` (PK.)  
  - `category_name`  
  - `parent_id` (FK â†’ CATEGORIES.category_id)  

- **BRANDS**  
  - `brand_id` (PK.)  
  - `brand_name`  

- **SESSIONS**  
  - `session_id` (PK.)  
  - `user_id` (FK â†’ USERS.user_id)  
  - `device_type`  
  - `channel`  
  - `started_at`  

- **EVENTS**  
  - `event_id` (PK.)  
  - `session_id` (FK â†’ SESSIONS.session_id)  
  - `product_id` (FK â†’ PRODUCTS.product_id)  
  - `event_type` (ENUM: view, cart, purchase)  
  - `event_time`  
  - `price`  

**Ventajas del modelo OLTP:**

- Alta normalizaciÃ³n garantiza consistencia y evita duplicaciÃ³n  
- Relaciones referenciales para trazabilidad completa: usuario â†’ sesiÃ³n â†’ evento  
- Optimizado para escritura intensiva  
- Preparado para replicaciÃ³n CDC mediante AWS DMS hacia S3  

---

### ğŸ§± Modelo AnalÃ­tico (Data Lake)

Una vez en S3, se aplica un proceso ETL para construir un modelo de datos orientado a anÃ¡lisis.

#### Modelo en estrella optimizado para Athena / Redshift:

- **Tabla de hechos:** `fact_user_events`  
  - `event_id`, `event_time`, `event_type`  
  - `user_id`, `product_id`, `price`, `session_id`  
  - `category_name`, `brand` (denormalizados)  
  - `device_type`, `source_channel`, `day_of_week`, `hour_of_day`  

- **Dimensiones:**  
  - `dim_users`: Perfil de usuario  
  - `dim_time`: Calendario  

**Capas del Data Lake en S3:**

- `raw/`: ingestiÃ³n bruta desde DMS  
- `clean/`: datos validados, transformados  
- `model/`: modelo en estrella en formato Parquet  

---

### âš™ï¸ Herramientas y TecnologÃ­as Elegidas

| Componente                  | TecnologÃ­a                         | Motivo de elecciÃ³n                                                   |
|----------------------------|-------------------------------------|----------------------------------------------------------------------|
| Base de datos OLTP         | Aurora PostgreSQL                   | Escalable, transaccional, ideal para app mÃ³vil                      |
| ReplicaciÃ³n continua       | AWS DMS (CDC)                       | Sincroniza datos sin afectar OLTP                                   |
| Almacenamiento             | Amazon S3                           | EconÃ³mico, escalable, nativo para Data Lake                         |
| TransformaciÃ³n             | AWS Glue + PySpark                  | Procesamiento distribuido sobre alto volumen                        |
| OrganizaciÃ³n de datos      | Data Lake por capas (raw-clean-model)| Mejora trazabilidad, modularidad y control                          |
| Consulta analÃ­tica         | Amazon Athena                       | SQL serverless, bajo costo, ideal para exploraciÃ³n y BI             |
| VisualizaciÃ³n              | Amazon QuickSight, Power BI         | IntegraciÃ³n directa con Athena y Redshift                           |
| Formato de almacenamiento  | Parquet                             | Columnar, comprimido, altamente eficiente en anÃ¡lisis               |

Ver arquitectura: [`architecture/architecture.png`](architecture/architecture.png)

---

### ğŸ” Frecuencia de ActualizaciÃ³n Recomendada

**Propuesta:** ActualizaciÃ³n cada **5 minutos** mediante micro-batches.

**JustificaciÃ³n:**

- El volumen y frecuencia de eventos exige **frescura cuasi real-time**  
- AWS DMS permite replicaciÃ³n continua desde Aurora hacia S3  
- AWS Glue puede ejecutarse por cron o triggers automÃ¡ticos en llegada de datos  
- Tablas como `dim_users` y `dim_products` pueden actualizarse **diariamente** o bajo cambios (SCD)

---

### âœ… ConclusiÃ³n

Esta arquitectura permite:

- Separar las cargas OLTP de las analÃ­ticas, preservando rendimiento  
- Ingestar y transformar datos a gran escala sin impacto en producciÃ³n  
- Ejecutar dashboards y modelos de anÃ¡lisis con datos frescos y organizados  
- Evolucionar fÃ¡cilmente hacia Redshift o Snowflake si la carga lo requiere  

La soluciÃ³n cumple con las mejores prÃ¡cticas de AWS para arquitectura analÃ­tica moderna, aplicando herramientas serverless, formatos columnarizados, y un modelo escalable sin dependencias innecesarias.

## ğŸ§© Paso 4: ConstrucciÃ³n del ETL

Estructurado en mÃ³dulos:

```bash
src/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py          # ConfiguraciÃ³n central de logs
â”‚   â””â”€â”€ exceptions.py      # Manejo de errores
â””â”€â”€ config/
    â””â”€â”€ settings.py        # Paths y parÃ¡metros globales
