# üõí eCommerce Data Pipeline

Dise√±o e implementaci√≥n de un pipeline moderno de datos en AWS para capturar, procesar y analizar eventos de comportamiento de usuarios en una tienda de comercio electr√≥nico.

## üß© Paso 1: Alcance del Proyecto y Captura de Datos

### üéØ Objetivo

Dise√±ar una arquitectura escalable de datos para una empresa de comercio electr√≥nico, enfocada en capturar, procesar y estructurar eventos de comportamiento de usuarios (como vistas de producto, adiciones al carrito y compras). El objetivo es habilitar flujos de valor anal√≠tico para **nutrir de datos a toda la compa√±√≠a**, soportando operaciones, Business Intelligence (BI) y Ciencia de Datos (DS).

Este ejercicio simula, a partir de un conjunto de datos de ejemplo descargado desde Kaggle, el dise√±o e implementaci√≥n de un pipeline de datos moderno: desde la ingesta hasta el modelado anal√≠tico, aplicando buenas pr√°cticas de calidad, gobierno y rendimiento sobre una arquitectura en AWS.

---

### üìÅ Dataset

- **Nombre:** eCommerce behavior data from multi category store  
- **Fuente:** [Kaggle - eCommerce behavior data](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store) 
- **Archivo utilizado:** `2020-Apr.csv.gz`  
- **Tama√±o:** 66.589.268 registros  
- **Formato:** CSV comprimido (.gz)  
- **Frecuencia:** Abril 2020  

**Campos principales:**

- `event_time`: Fecha y hora del evento  
- `event_type`: Tipo de evento (`view`, `cart`, `remove_from_cart`, `purchase`)  
- `product_id`: ID del producto  
- `category_id`: ID de la categor√≠a  
- `category_code`: Categor√≠a espec√≠fica del producto  
- `brand`: Marca del producto  
- `price`: Precio del producto  
- `user_id`: ID an√≥nimo del usuario  
- `user_session`: ID de la sesi√≥n de navegaci√≥n  

üìå *Limitaciones conocidas:*  
El dataset solo representa un mes de eventos, y las sesiones de usuario son an√≥nimas. No incluye datos de usuarios autenticados ni eventos offline.

---

### üì¶ Captura de Datos

El archivo fue descargado desde Kaggle, descomprimido y ubicado en la ruta del proyecto:

```
/datos/raw/2020-Abr.csv.gz
```

Este ser√° utilizado como **fuente principal de datos** para construir el pipeline.

---

### üîç Casos de Uso Esperados

Este proyecto representa una soluci√≥n de datos para un **eCommerce** que busca consolidar una plataforma anal√≠tica empresarial capaz de alimentar diferentes √°reas:

#### üß† Para la organizaci√≥n

- Plataforma de datos unificada y gobernada  
- Integraci√≥n entre equipos de marketing, operaciones, BI y ciencia de datos  
- Toma de decisiones basada en datos reales del comportamiento de usuarios  

#### üìä Business Intelligence (BI)

- Embudo de conversi√≥n: `view ‚Üí cart ‚Üí purchase`  
- Ranking de productos con mayor visualizaci√≥n vs. menor conversi√≥n  
- An√°lisis de sesiones por usuario y duraci√≥n promedio  
- Top categor√≠as y marcas por volumen de ventas  
- Evoluci√≥n de eventos por hora, d√≠a y semana  
- Comparativo de precios promedio por categor√≠a y marca  
- Identificaci√≥n de usuarios m√°s activos y recurrentes  

#### üî¨ Ciencia de Datos (Data Science)

- Segmentaci√≥n de usuarios basada en comportamiento  
- Predicci√≥n de probabilidad de compra  
- Detecci√≥n de anomal√≠as en precios o eventos  
- Clustering de productos por interacci√≥n y conversi√≥n  
- Modelos de propensi√≥n al abandono de carrito  
- Sistemas de recomendaci√≥n personalizados  

---

## üìä Paso 2: Exploraci√≥n y Evaluaci√≥n de Datos (EDA)

Para analizar un dataset de m√°s de **66 millones de registros**, se utiliz√≥ **PySpark** como motor de procesamiento distribuido. Gracias a su escalabilidad, se pudieron ejecutar transformaciones complejas y validaciones sin saturar el entorno de desarrollo.

Se tom√≥ una muestra aleatoria de aproximadamente **1.5 millones de registros** (~2.3% del total), lo que permiti√≥ realizar un **an√°lisis exploratorio eficiente** preservando la diversidad de tipos de eventos, productos y usuarios.

El an√°lisis exploratorio se realiz√≥ utilizando PySpark y se document√≥ en el notebook 

```
/notebooks/eda.ipynb.
```

---

### üîç Exploraci√≥n: Calidad de los Datos

#### üìå A. Valores Nulos Detectados

| Columna          | Valores Nulos |
|------------------|----------------|
| `brand`          | 205,961        |
| `category_code`  | 155,385        |
| `user_session`   | 2              |
| Resto de columnas| 0              |

#### üìå B. Registros Duplicados

- Total registros: **1,531,767**
- Registros √∫nicos: **1,531,619**
- **Duplicados detectados:** 148

#### üìå C. Valores √önicos por Columna

- `user_id`: 892,389
- `product_id`: 124,922
- `brand`: 3,661
- `category_code`: 139
- `event_time`: 1,085,863
- `user_session`: 1,281,641

#### üìå D. Tipos de Evento

| Tipo de evento | Registros |
|----------------|-----------|
| `view`         | 1,434,849 |
| `cart`         | 74,880    |
| `purchase`     | 22,038    |

‚Üí Representaci√≥n t√≠pica del embudo de conversi√≥n eCommerce: vistas > carritos > compras.

#### üìå E. Estad√≠sticas del Precio

| M√©trica   | Valor       |
|-----------|-------------|
| Count     | 1,531,767   |
| Media     | $273.11     |
| Desviaci√≥n est√°ndar | $356.13 |
| M√≠nimo    | $0.00       |
| M√°ximo    | $2,574.07   |

---

### üßº Sugerencias para la Limpieza de Datos

A partir de los hallazgos previos, se proponen las siguientes estrategias de limpieza para mejorar la calidad de los datos antes del modelado:

1. **Conversi√≥n de tipos**
   - Convertir `event_time` a `timestamp` con zona horaria UTC.
   - Tipificar `event_type` como variable categ√≥rica controlada (`view`, `cart`, `purchase`).

2. **Eliminaci√≥n de duplicados**
   - Remover registros completamente duplicados (id√©nticos en todas las columnas).

3. **Tratamiento de valores nulos**
   - Imputar `brand` y `category_code` con `"unknown"` cuando el porcentaje de nulos por categor√≠a sea bajo (<5%).
   - Omitir registros con `user_session` nulo (2 casos identificados).

4. **Filtrado de precios inv√°lidos**
   - Excluir registros con precio igual a 0 o negativo, ya que no representan comportamiento v√°lido.

5. **Verificaci√≥n de relaciones l√≥gicas**
   - Validar consistencia entre `user_id`, `user_session` y secuencia de `event_type`.
   - Confirmar flujos completos del embudo de conversi√≥n en sesiones (`view ‚Üí cart ‚Üí purchase`).

---

### üß™ Justificaci√≥n del Muestreo y Uso de PySpark

- üß† **Muestreo controlado (~1.5M filas)**: permite acelerar el desarrollo local sin sacrificar representatividad estad√≠stica.
- üî• **PySpark**: motor de procesamiento distribuido ideal para trabajar con datasets a gran escala como el original (66M+ registros), habilitando limpieza, transformaci√≥n y an√°lisis eficiente sobre AWS Glue u otros entornos.

---

## üß© Paso 3: Definici√≥n del Modelo de Datos y Arquitectura

### üß≠ Contexto

En un entorno de eCommerce moderno, los usuarios interact√∫an con una aplicaci√≥n m√≥vil generando millones de eventos mensuales (vistas de producto, adiciones al carrito, compras, etc.). Estos eventos son almacenados inicialmente en una **base de datos transaccional (OLTP)** como Aurora PostgreSQL, optimizada para escritura y consistencia. A partir de all√≠, se construye un pipeline para transformar y preparar los datos para uso anal√≠tico, dashboards de BI y ciencia de datos.

---

### üóÇÔ∏è Modelo de Datos Conceptual (OLTP)

#### Dise√±o inicial en Aurora PostgreSQL

La base de datos transaccional se dise√±√≥ utilizando un modelo **normalizado** con integridad referencial, ideal para registrar actividad desde la app m√≥vil en tiempo real.

**Entidades principales (PK y FK):**

- **USERS**  
  - `user_id` (PK)  
  - `name`  
  - `email`  
  - `created_at`  

- **PRODUCTS**  
  - `product_id` (PK)  
  - `name`  
  - `brand_id` (FK ‚Üí BRANDS.brand_id)  
  - `category_id` (FK ‚Üí CATEGORIES.category_id)  
  - `price`  
  - `stock`  

- **CATEGORIES**  
  - `category_id` (PK)  
  - `category_name`  
  - `parent_id` (FK ‚Üí CATEGORIES.category_id)  

- **BRANDS**  
  - `brand_id` (PK)  
  - `brand_name`  

- **SESSIONS**  
  - `session_id` (PK)  
  - `user_id` (FK ‚Üí USERS.user_id)  
  - `device_type`  
  - `channel`  
  - `started_at`  

- **EVENTS**  
  - `event_id` (PK)  
  - `session_id` (FK ‚Üí SESSIONS.session_id)  
  - `product_id` (FK ‚Üí PRODUCTS.product_id)  
  - `event_type` (ENUM: view, cart, purchase)  
  - `event_time`  
  - `price`  

**Ventajas del modelo OLTP:**

- Alta normalizaci√≥n garantiza consistencia y evita duplicaci√≥n  
- Relaciones referenciales para trazabilidad completa: usuario ‚Üí sesi√≥n ‚Üí evento  
- Optimizado para escritura intensiva  
- Preparado para replicaci√≥n CDC mediante AWS DMS hacia S3  

---

### üß± Modelo Anal√≠tico (Data Lake)

Una vez en S3, se aplica un proceso ETL para construir un modelo de datos orientado a an√°lisis.

#### Modelo en estrella optimizado para Athena / Redshift:

- **Tabla de hechos:** `fact_user_events`  
  - `event_id`, `event_time`, `event_type`  
  - `user_id`, `product_id`, `price`, `session_id`  
  - `category_name`, `brand` (denormalizados)  
  - `device_type`, `source_channel`, `day_of_week`, `hour_of_day`  

- **Dimensiones:**  
  - `dim_users`: Perfil de usuario  
  - `dim_time`: Calendario  

**Capas del Data Lake en S3:**

- `raw/`: ingesti√≥n bruta desde DMS  
- `clean/`: datos validados, transformados  
- `model/`: modelo en estrella en formato Parquet  

---

### ‚öôÔ∏è Herramientas y Tecnolog√≠as Elegidas

| Componente                  | Tecnolog√≠a                         | Motivo de elecci√≥n                                                   |
|----------------------------|-------------------------------------|----------------------------------------------------------------------|
| Base de datos OLTP         | Aurora PostgreSQL                   | Escalable, transaccional, ideal para app m√≥vil                      |
| Replicaci√≥n continua       | AWS DMS (CDC)                       | Sincroniza datos sin afectar OLTP                                   |
| Almacenamiento             | Amazon S3                           | Econ√≥mico, escalable, nativo para Data Lake                         |
| Transformaci√≥n             | AWS Glue + PySpark                  | Procesamiento distribuido sobre alto volumen                        |
| Organizaci√≥n de datos      | Data Lake por capas (raw-clean-model)| Mejora trazabilidad, modularidad y control                          |
| Consulta anal√≠tica         | Amazon Athena                       | SQL serverless, bajo costo, ideal para exploraci√≥n y BI             |
| Visualizaci√≥n              | Amazon QuickSight, Power BI         | Integraci√≥n directa con Athena y Redshift                           |
| Formato de almacenamiento  | Parquet                             | Columnar, comprimido, altamente eficiente en an√°lisis               |

---

### üîÅ Frecuencia de Actualizaci√≥n Recomendada

**Propuesta:** Actualizaci√≥n cada **5 minutos** mediante micro-batches.

**Justificaci√≥n:**

- El volumen y frecuencia de eventos exige **frescura cuasi real-time**  
- AWS DMS permite replicaci√≥n continua desde Aurora hacia S3  
- AWS Glue puede ejecutarse por cron o triggers autom√°ticos en llegada de datos  
- Tablas como `dim_users` y `dim_products` pueden actualizarse **diariamente** o bajo cambios (SCD)

---

### ‚úÖ Conclusi√≥n

Esta arquitectura permite:

- Separar las cargas OLTP de las anal√≠ticas, preservando rendimiento  
- Ingestar y transformar datos a gran escala sin impacto en producci√≥n  
- Ejecutar dashboards y modelos de an√°lisis con datos frescos y organizados  
- Evolucionar f√°cilmente hacia Redshift o Snowflake si la carga lo requiere  

La soluci√≥n cumple con las mejores pr√°cticas de AWS para arquitectura anal√≠tica moderna, aplicando herramientas serverless, formatos columnarizados, y un modelo escalable sin dependencias innecesarias.

## üß© Paso 4: Construcci√≥n del ETL

Estructurado en m√≥dulos:

```bash
src/
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ extract.py
‚îÇ   ‚îú‚îÄ‚îÄ transform.py
‚îÇ   ‚îî‚îÄ‚îÄ load.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ logger.py          # Configuraci√≥n central de logs
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py      # Manejo de errores
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ settings.py        # Paths y par√°metros globales
