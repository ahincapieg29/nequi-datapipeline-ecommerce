# üõí eCommerce Data Pipeline

Dise√±o e implementaci√≥n de un pipeline moderno de datos en AWS para capturar, procesar y analizar eventos de comportamiento de usuarios en una tienda de comercio electr√≥nico.

## üß© Paso 1: Alcance del Proyecto y Captura de Datos

### üéØ Objetivo

Dise√±ar una arquitectura escalable de datos para una empresa de comercio electr√≥nico, enfocada en capturar, procesar y estructurar eventos de comportamiento de usuarios (como vistas de producto, adiciones al carrito y compras). El objetivo es habilitar flujos de valor anal√≠tico para **nutrir de datos a toda la compa√±√≠a**, soportando operaciones, Business Intelligence (BI) y Ciencia de Datos (DS).

Este ejercicio simula, a partir de un conjunto de datos de ejemplo descargado desde Kaggle, el dise√±o e implementaci√≥n de un pipeline de datos moderno: desde la ingesta hasta el modelado anal√≠tico, aplicando buenas pr√°cticas de calidad, gobierno y rendimiento sobre una arquitectura en AWS.

---

### üìÅ Dataset

Este dataset fue elegido porque simula un entorno real de eCommerce con m√∫ltiples categor√≠as, eventos, usuarios y sesiones, lo que permite aplicar t√©cnicas modernas de modelado de eventos, trazabilidad y construcci√≥n de embudos. Adem√°s, su volumen (66M+) lo convierte en un excelente candidato para probar escalabilidad y rendimiento en arquitectura cloud.
En un escenario productivo, se espera que los eventos se capturen en tiempo casi real o por lotes horarios para soportar decisiones operacionales y anal√≠ticas de forma oportuna.

- **Nombre:** eCommerce behavior data from multi category store  
- **Fuente:** [Kaggle - eCommerce behavior data](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store) 
- **Archivo utilizado:** `2020-Apr.csv.gz`  
- **Tama√±o:** 66.589.268 registros  
- **Formato:** CSV comprimido (.gz)  
- **Frecuencia:** Abril 2020  

**Campos principales:**

- `event_time`: Fecha y hora del evento  
- `event_type`: Tipo de evento (`view`, `cart`, `remove_from_cart`, `purchase`)  
- `product_id`: ID del producto  
- `category_id`: ID de la categor√≠a  
- `category_code`: Categor√≠a espec√≠fica del producto  
- `brand`: Marca del producto  
- `price`: Precio del producto  
- `user_id`: ID an√≥nimo del usuario  
- `user_session`: ID de la sesi√≥n de navegaci√≥n  

üìå *Limitaciones conocidas:*  
El dataset solo representa un mes de eventos, y las sesiones de usuario son an√≥nimas. No incluye datos de usuarios autenticados ni eventos offline.

---

### üì¶ Captura de Datos

El archivo fue descargado desde Kaggle, descomprimido y ubicado en la ruta del proyecto:

```
/datos/raw/2020-Abr.csv.gz
```

Este ser√° utilizado como **fuente principal de datos** para construir el pipeline.

---

### üîç Casos de Uso Esperados

Este proyecto representa una soluci√≥n de datos para un **eCommerce** que busca consolidar una plataforma anal√≠tica empresarial capaz de alimentar diferentes √°reas:

#### üß† Para la organizaci√≥n

- Plataforma de datos unificada y gobernada  
- Integraci√≥n entre equipos de marketing, operaciones, BI y ciencia de datos  
- Toma de decisiones basada en datos reales del comportamiento de usuarios  

#### üß™ Business Intelligence (BI)

- Embudo de conversi√≥n: `view ‚Üí cart ‚Üí purchase`  
- Ranking de productos con mayor visualizaci√≥n vs. menor conversi√≥n  
- An√°lisis de sesiones por usuario y duraci√≥n promedio  
- Top categor√≠as y marcas por volumen de ventas  
- Evoluci√≥n de eventos por hora, d√≠a y semana  
- Comparativo de precios promedio por categor√≠a y marca  
- Identificaci√≥n de usuarios m√°s activos y recurrentes  

#### üî¨ Ciencia de Datos (Data Science)

- Segmentaci√≥n de usuarios basada en comportamiento  
- Predicci√≥n de probabilidad de compra  
- Detecci√≥n de anomal√≠as en precios o eventos  
- Clustering de productos por interacci√≥n y conversi√≥n  
- Modelos de propensi√≥n al abandono de carrito  
- Sistemas de recomendaci√≥n personalizados  

---

## üß© Paso 2: Exploraci√≥n y Evaluaci√≥n de Datos (EDA)

Para analizar un dataset de m√°s de **66 millones de registros**, se utiliz√≥ **PySpark** como motor de procesamiento distribuido. Gracias a su escalabilidad, se pudieron ejecutar transformaciones complejas y validaciones sin saturar el entorno de desarrollo.

Se tom√≥ una muestra aleatoria de aproximadamente **1.5 millones de registros** (~2.3% del total), lo que permiti√≥ realizar un **an√°lisis exploratorio eficiente** preservando la diversidad de tipos de eventos, productos y usuarios.

El an√°lisis exploratorio se realiz√≥ utilizando PySpark y se document√≥ en el notebook 

```
/notebooks/eda.ipynb.
```

---

### üîç Exploraci√≥n: Calidad de los Datos

#### üìå A. Valores Nulos Detectados

| Columna          | Valores Nulos |
|------------------|----------------|
| `brand`          | 205,961        |
| `category_code`  | 155,385        |
| `user_session`   | 2              |
| Resto de columnas| 0              |

#### üìå B. Registros Duplicados

- Total registros: **1,531,767**
- Registros √∫nicos: **1,531,619**
- **Duplicados detectados:** 148

#### üìå C. Valores √önicos por Columna

- `user_id`: 892,389
- `product_id`: 124,922
- `brand`: 3,661
- `category_code`: 139
- `event_time`: 1,085,863
- `user_session`: 1,281,641

#### üìå D. Tipos de Evento

| Tipo de evento | Registros |
|----------------|-----------|
| `view`         | 1,434,849 |
| `cart`         | 74,880    |
| `purchase`     | 22,038    |

‚Üí Representaci√≥n t√≠pica del embudo de conversi√≥n eCommerce: vistas > carritos > compras.

#### üìå E. Estad√≠sticas del Precio

| M√©trica   | Valor       |
|-----------|-------------|
| Count     | 1,531,767   |
| Media     | $273.11     |
| Desviaci√≥n est√°ndar | $356.13 |
| M√≠nimo    | $0.00       |
| M√°ximo    | $2,574.07   |

---

### üßº Sugerencias para la Limpieza de Datos

A partir de los hallazgos previos, se proponen las siguientes estrategias de limpieza para mejorar la calidad de los datos antes del modelado:

```
Raw data (CSV) 
    ‚Üì
Remove duplicates
    ‚Üì
Handle nulls
    ‚Üì
Filter invalid prices
    ‚Üì
Validate event logic
    ‚Üì
‚Üí Cleaned dataset
```


1. **Conversi√≥n de tipos**
   - Convertir `event_time` a `timestamp` con zona horaria UTC.
   - Tipificar `event_type` como variable categ√≥rica controlada (`view`, `cart`, `purchase`).

2. **Eliminaci√≥n de duplicados**
   - Remover registros completamente duplicados (id√©nticos en todas las columnas).

3. **Tratamiento de valores nulos**
   - Imputar `brand` y `category_code` con `"unknown"` cuando el porcentaje de nulos por categor√≠a sea bajo (<5%).
   - Omitir registros con `user_session` nulo (2 casos identificados).

4. **Filtrado de precios inv√°lidos**
   - Excluir registros con precio igual a 0 o negativo, ya que no representan comportamiento v√°lido.

5. **Verificaci√≥n de relaciones l√≥gicas**
   - Validar consistencia entre `user_id`, `user_session` y secuencia de `event_type`.
   - Confirmar flujos completos del embudo de conversi√≥n en sesiones (`view ‚Üí cart ‚Üí purchase`).

---

### üß™ Justificaci√≥n del Muestreo y Uso de PySpark

- üß† **Muestreo controlado (~1.5M filas)**: permite acelerar el desarrollo local sin sacrificar representatividad estad√≠stica.
- üî• **PySpark**: motor de procesamiento distribuido ideal para trabajar con datasets a gran escala como el original (66M+ registros), habilitando limpieza, transformaci√≥n y an√°lisis eficiente sobre AWS Glue u otros entornos.

---

## üß© Paso 3: Definici√≥n del Modelo de Datos y Arquitectura

### üß≠ Contexto

En un entorno de eCommerce moderno, los usuarios interact√∫an con una aplicaci√≥n m√≥vil generando millones de eventos mensuales (vistas de producto, adiciones al carrito, compras, etc.). Estos eventos son almacenados inicialmente en una **base de datos transaccional (OLTP)** como Aurora PostgreSQL, optimizada para escritura y consistencia. A partir de all√≠, se construye un pipeline para transformar y preparar los datos para uso anal√≠tico, dashboards de BI y ciencia de datos.

---

### üóÇÔ∏è Modelo de Datos Conceptual (OLTP)

#### Dise√±o inicial en Aurora PostgreSQL

La base de datos transaccional se dise√±√≥ utilizando un modelo **normalizado** con integridad referencial, ideal para registrar actividad desde la app m√≥vil en tiempo real.

**Entidades principales (PK y FK):**

- **USERS**  
  - `user_id` (Primary Key)  
  - `name`  
  - `email`  
  - `created_at`  

- **PRODUCTS**  
  - `product_id` (Primary Key)  
  - `name`  
  - `brand_id` (Foreign Key ‚Üí BRANDS.brand_id)  
  - `category_id` (Foreign Key ‚Üí CATEGORIES.category_id)  
  - `price`  
  - `stock`  

- **CATEGORIES**  
  - `category_id` (Primary Key)  
  - `category_name`  
  - `parent_id` (Foreign Key ‚Üí CATEGORIES.category_id)  

- **BRANDS**  
  - `brand_id` (Primary Key)  
  - `brand_name`  

- **SESSIONS**  
  - `session_id` (Primary Key)  
  - `user_id` (Foreign Key ‚Üí USERS.user_id)  
  - `device_type`  
  - `channel`  
  - `started_at`  

- **EVENTS**  
  - `event_id` (Primary Key)  
  - `session_id` (Foreign Key ‚Üí SESSIONS.session_id)  
  - `product_id` (Foreign Key ‚Üí PRODUCTS.product_id)  
  - `event_type` (ENUM: view, cart, purchase)  
  - `event_time`  
  - `price`  

**Ventajas del modelo OLTP:**

- Alta normalizaci√≥n garantiza consistencia y evita duplicaci√≥n  
- Relaciones referenciales para trazabilidad completa: usuario ‚Üí sesi√≥n ‚Üí evento  
- Optimizado para escritura intensiva  
- Preparado para replicaci√≥n CDC mediante AWS DMS hacia S3  

---

### üß± Modelo Anal√≠tico (Data Lake)

Una vez en S3, se aplica un proceso ETL para construir un modelo de datos orientado a an√°lisis.

#### Modelo en estrella optimizado para Athena / Redshift:

- **Tabla de hechos:** `fact_user_events`  
  - `event_id`, `event_time`, `event_type`  
  - `user_id`, `product_id`, `price`, `session_id`  
  - `category_name`, `brand` (denormalizados)  
  - `device_type`, `source_channel`, `day_of_week`, `hour_of_day`  

- **Dimensiones:**  
  - `dim_users`: Perfil de usuario  
  - `dim_products`: Productos  

**Capas del Data Lake en S3:**

- `raw/`: ingesti√≥n bruta desde DMS  
- `clean/`: datos validados, transformados  
- `model/`: modelo en estrella en formato Parquet  

---

### ‚öôÔ∏è Herramientas y Tecnolog√≠as Elegidas

| Componente                  | Tecnolog√≠a                         | Motivo de elecci√≥n                                                   |
|----------------------------|-------------------------------------|----------------------------------------------------------------------|
| Base de datos OLTP         | Aurora PostgreSQL                   | Escalable, transaccional, ideal para app m√≥vil                      |
| Replicaci√≥n continua       | AWS DMS (CDC)                       | Sincroniza datos sin afectar OLTP                                   |
| Almacenamiento             | Amazon S3                           | Econ√≥mico, escalable, nativo para Data Lake                         |
| Transformaci√≥n             | AWS Glue + PySpark                  | Procesamiento distribuido sobre alto volumen                        |
| Organizaci√≥n de datos      | Data Lake por capas (raw-clean-model)| Mejora trazabilidad, modularidad y control                          |
| Consulta anal√≠tica         | Athena                              | SQL serverless, bajo costo, ideal para exploraci√≥n y BI             |
| Visualizaci√≥n              | Amazon QuickSight, Power BI         | Integraci√≥n directa con Athena y Redshift                           |
| Formato de almacenamiento  | Parquet                             | Columnar, comprimido, altamente eficiente en an√°lisis               |

```
[Mobile App]
     ‚Üì
[Aurora PostgreSQL] --(CDC)--> [AWS DMS]
                                ‚Üì
                          [S3 - raw/]
                                ‚Üì
                     [AWS Glue + PySpark]
                                ‚Üì
                        [S3 - clean/model]
                                ‚Üì
                            [Athena]
                              ‚Üì
                   [Power BI / QuickSight ]
```


- AWS Glue fue elegido sobre Lambda + Step Functions porque el volumen de datos (66M+) y las transformaciones requeridas (join, filtrado, particionado) se benefician del procesamiento distribuido con PySpark.
- Aurora PostgreSQL permite escalabilidad transaccional con r√©plicas, ideal para integraci√≥n con CDC (Change Data Capture) usando AWS DMS.
- S3 es el almacenamiento √≥ptimo para un Data Lake escalable, y permite separaci√≥n por capas (`raw`, `clean`, `model`) con esquemas evolutivos.
- La soluci√≥n incluye monitoreo de jobs de Glue mediante CloudWatch y validaciones de ingesta en Athena para asegurar integridad de datos en cada ciclo de actualizaci√≥n.

---

### üîÅ Frecuencia de Actualizaci√≥n Recomendada

**Propuesta:** Actualizaci√≥n cada **1 hora** mediante **microlotes** para la tabla de hechos `fact_user_events`, y cargas **diarias** para dimensiones maestras (`dim_users`, `dim_products`).

**Justificaci√≥n:**

#### Para Business Intelligence (BI):
- Una actualizaci√≥n **cada hora** es suficiente para:
  - Monitorear comportamiento de usuarios en tiempo operativo
  - Medir rendimiento de campa√±as activas sin necesidad de real-time
  - Mantener dashboards √°giles con bajo costo computacional
  - Compatible con Power BI, QuickSight y Athena (consulta sobre particiones por fecha).

#### Para Ciencia de Datos (DS):
- Cargas **diarias** permiten:
  - Entrenamiento eficiente de modelos predictivos y an√°lisis exploratorio
  - Preparaci√≥n de features hist√≥ricas para clustering, scoring y segmentaci√≥n
  - Menor carga operativa y m√°s estabilidad en pipelines de entrenamiento

#### Capacidad t√©cnica:
- **AWS DMS** permite replicaci√≥n continua desde Aurora PostgreSQL hacia S3 (`raw/`).
- **AWS Glue** se puede ejecutar por cron cada hora para transformar solo los nuevos datos del d√≠a (`PROCESS_DATE=HOY`).
- El particionado por `event_date` permite cargas y consultas optimizadas en Athena y Redshift Spectrum.

---

### ‚úÖ Conclusi√≥n

Esta arquitectura permite:

- Separar las cargas OLTP de las anal√≠ticas, preservando rendimiento  
- Ingestar y transformar datos a gran escala sin impacto en producci√≥n  
- Ejecutar dashboards y modelos de an√°lisis con datos frescos y organizados  
- Evolucionar f√°cilmente hacia Redshift o Snowflake si la carga lo requiere  

La soluci√≥n cumple con las mejores pr√°cticas de AWS para arquitectura anal√≠tica moderna, aplicando herramientas serverless, formatos columnarizados, y un modelo escalable sin dependencias innecesarias.

---

# üß© Paso 4: Construcci√≥n del Pipeline ETL

Esta etapa implementa un pipeline escalable y modular que procesa eventos desde una base de datos OLTP en **Aurora PostgreSQL** (v√≠a **CDC con AWS DMS**) hasta un modelo anal√≠tico en **S3** en formato **Parquet**, listo para consultas con **Athena** o visualizaci√≥n en **Power BI / QuickSight**.

El pipeline transforma datos crudos en entidades anal√≠ticas estructuradas, incluyendo limpieza, enriquecimiento, validaciones y pruebas automatizadas para asegurar calidad y trazabilidad.

---

## üèóÔ∏è Arquitectura T√©cnica

```
[App M√≥vil]
     ‚Üì
[Aurora PostgreSQL] ‚Üí [AWS DMS (CDC)]
     ‚Üì
[S3 (raw/)]
     ‚Üì
[AWS Glue (PySpark ETL)]
     ‚Üì
[S3 (clean/)]
     ‚Üì
[S3 (model/fact_user_events, dim_*)]
     ‚Üì
[Athena / QuickSight / Power BI]
```

---

## üîÅ Frecuencia de Procesamiento

| Tabla               | Frecuencia | Detalle                                               |
|---------------------|------------|--------------------------------------------------------|
| `fact_user_events`  | Cada hora  | Microlote ‚Üí sobrescribe partici√≥n `event_date=HOY`     |
| `dim_users`         | Diaria     | Carga completa desde `clean/`                          |
| `dim_products`      | Diaria     | Carga completa desde `clean/`                          |

---

## üóÇÔ∏è Estructura del Repositorio

```bash
/etl/
‚îú‚îÄ‚îÄ extract/
‚îÇ   ‚îî‚îÄ‚îÄ extract_from_s3.py              # Lectura de eventos del d√≠a desde raw/
‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îú‚îÄ‚îÄ clean_and_transform_events.py   # Limpieza ‚Üí guarda en clean/
‚îÇ   ‚îî‚îÄ‚îÄ transform_dimensions.py         # Lee clean/, genera dimensiones
‚îú‚îÄ‚îÄ load/
‚îÇ   ‚îî‚îÄ‚îÄ load_to_model.py                # Escritura final en model/
‚îú‚îÄ‚îÄ quality/
‚îÇ   ‚îî‚îÄ‚îÄ quality_checks.py               # Validaciones generales
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ spark_session.py                # Instancia de Spark (Glue o local)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ unit_tests_etl.py               # Pruebas unitarias automatizadas
‚îî‚îÄ‚îÄ run_etl.py                          # Orquestador principal del proceso horario
```

---

## ‚úÖ Ejecuci√≥n ETL Horaria

### `run_etl.py`

```python
"""
Ejecuci√≥n principal del pipeline:
1. Extrae eventos diarios desde S3/raw
2. Limpia, transforma y valida datos ‚Üí clean/
3. Carga hechos en model/
4. Compara conteos entre capas
"""

from extract.extract_from_s3 import extract_events
from transform.clean_and_transform_events import clean_transform
from load.load_to_model import load_events
from quality.quality_checks import compare_counts_between_layers
from utils.spark_session import get_spark_session
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    try:
        logger.info("üöÄ Iniciando ETL de eventos (horaria)")
        spark = get_spark_session("ETL-Hourly")
        df_raw = extract_events()
        df_clean = clean_transform(df_raw)
        load_events(df_clean)
        compare_counts_between_layers(spark)
        logger.info("‚úÖ ETL completada correctamente")
    except Exception as e:
        logger.error(f"‚ùå Error en la ETL: {e}")
        raise
```

---

### `extract/extract_from_s3.py`

```python
"""
Extrae eventos del d√≠a actual desde la capa raw/ en S3.
"""

from utils.spark_session import get_spark_session
from pyspark.sql.functions import current_date
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_events():
    spark = get_spark_session("ExtractEvents")
    logger.info("üì• Extrayendo eventos de S3/raw/")
    df = spark.read.parquet("s3://ecommerce-lake/raw/events/")
    return df.filter(df.event_date == current_date())
```

---

### `transform/clean_and_transform_events.py`

```python
"""
Limpia eventos, elimina duplicados, filtra precios inv√°lidos, imputa valores nulos,
agrega columnas temporales y escribe en clean/.
"""

from pyspark.sql.functions import col, hour, dayofweek, sha2, concat_ws
from quality.quality_checks import (
    check_row_counts, check_nulls, check_uniqueness, check_schema
)
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

EXPECTED_COLUMNS = [
    "event_time", "event_type", "user_id", "product_id", 
    "category_code", "brand", "price", "user_session"
]

def clean_transform(df):
    logger.info("üßº Iniciando limpieza de eventos")
    check_schema(df, EXPECTED_COLUMNS)

    df = df.dropDuplicates()
    df = df.filter(col("price") > 0)
    df = df.fillna({"brand": "unknown", "category_code": "unknown"})
    df = df.filter(col("user_session").isNotNull())
    df = df.withColumn("hour_of_day", hour("event_time")) \
           .withColumn("day_of_week", dayofweek("event_time"))

    # Generaci√≥n de ID √∫nico para el evento
    df = df.withColumn("event_id", sha2(concat_ws("-", "user_id", "product_id", "event_time"), 256))

    # Validaciones
    check_row_counts(df, 10000)
    check_nulls(df, ["event_time", "event_type", "user_id", "product_id"])
    check_uniqueness(df, "event_id")

    df.write.mode("overwrite").partitionBy("event_date").parquet("s3://ecommerce-lake/clean/events/")
    logger.info("üì§ Datos limpios escritos en clean/")
    return df
```

---

### `load/load_to_model.py`

```python
"""
Carga eventos limpios en model/fact_user_events/ particionando por event_date.
"""

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_events(df):
    path = "s3://ecommerce-lake/model/fact_user_events/"
    logger.info(f"üíæ Guardando eventos en modelo anal√≠tico: {path}")
    df.write.mode("overwrite").partitionBy("event_date").parquet(path)
```

---

### `transform/transform_dimensions.py`

```python
"""
Carga diaria de dimensiones desde clean/ ‚Üí model/
"""

from utils.spark_session import get_spark_session
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def transform_and_load_dimensions():
    spark = get_spark_session("ETL-Daily")

    # Dimensi√≥n de usuarios
    logger.info("üë§ Procesando dim_users desde clean/")
    df_users = spark.read.parquet("s3://ecommerce-lake/clean/events/") \
        .select("user_id").dropna().dropDuplicates()
    df_users.write.mode("overwrite").parquet("s3://ecommerce-lake/model/dim_users/")
    logger.info("‚úÖ dim_users cargada")

    # Dimensi√≥n de productos
    logger.info("üì¶ Procesando dim_products desde clean/")
    df_products = spark.read.parquet("s3://ecommerce-lake/clean/events/") \
        .select("product_id", "brand", "category_code", "price") \
        .dropna(subset=["product_id", "price"]).dropDuplicates(["product_id"])
    df_products = df_products.fillna({"brand": "unknown", "category_code": "unknown"})
    df_products.write.mode("overwrite").parquet("s3://ecommerce-lake/model/dim_products/")
    logger.info("‚úÖ dim_products cargada")
```

---

## üîç Validaciones de Calidad: `quality/quality_checks.py`

```python
"""
Valida cantidad m√≠nima, nulos, unicidad, esquema esperado y p√©rdida de registros.
"""

from pyspark.sql.functions import col, approx_count_distinct, current_date

def check_row_counts(df, min_expected):
    count = df.count()
    assert count >= min_expected, f"‚ùå Solo {count} registros, m√≠nimo requerido: {min_expected}"

def check_nulls(df, cols):
    for c in cols:
        nulls = df.filter(col(c).isNull()).count()
        assert nulls == 0, f"‚ùå Nulls en columna {c}: {nulls}"

def check_uniqueness(df, col_name):
    total = df.count()
    unique = df.select(approx_count_distinct(col_name)).collect()[0][0]
    assert unique == total, f"‚ùå Duplicados detectados en {col_name}"

def check_schema(df, expected_cols):
    actual = set(df.columns)
    expected = set(expected_cols)
    missing = expected - actual
    extra = actual - expected
    assert not missing, f"‚ùå Faltan columnas: {missing}"
    if extra:
        print(f"‚ö†Ô∏è Columnas adicionales presentes: {extra}")

def compare_counts_between_layers(spark):
    today = current_date()
    raw = spark.read.parquet("s3://ecommerce-lake/raw/events/") \
        .filter(col("event_date") == today).count()
    model = spark.read.parquet("s3://ecommerce-lake/model/fact_user_events/") \
        .filter(col("event_date") == today).count()
    assert model >= raw * 0.98, f"‚ùå P√©rdida >2% entre RAW ({raw}) y MODEL ({model})"
```

---

## üß™ Tests Automatizados: `tests/unit_tests_etl.py`

```python
"""
Pruebas unitarias para funciones de validaci√≥n.
"""

import unittest
from quality import quality_checks
from pyspark.sql import SparkSession

class TestETL(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.spark = SparkSession.builder.master("local[*]").appName("ETLTest").getOrCreate()

    def test_check_row_counts(self):
        df = self.spark.createDataFrame([(1,), (2,)], ["id"])
        quality_checks.check_row_counts(df, 1)

    def test_check_nulls(self):
        df = self.spark.createDataFrame([(1,), (2,)], ["id"])
        quality_checks.check_nulls(df, ["id"])

    def test_check_uniqueness(self):
        df = self.spark.createDataFrame([(1,), (1,)], ["event_id"])
        with self.assertRaises(AssertionError):
            quality_checks.check_uniqueness(df, "event_id")

    def test_check_schema(self):
        df = self.spark.createDataFrame([(1, 2)], ["a", "b"])
        with self.assertRaises(AssertionError):
            quality_checks.check_schema(df, ["a", "b", "c"])

if __name__ == "__main__":
    unittest.main()
```

---

## ‚òÅÔ∏è Ejecuci√≥n en AWS Glue

| Job                  | Script                      | Frecuencia       | Trigger Cron             |
|----------------------|-----------------------------|------------------|---------------------------|
| ETL Horaria Eventos  | `run_etl.py`                | Cada hora        | `cron(0 * ? * * *)`       |
| Carga Diaria Dim     | `transform_dimensions.py`   | Cada d√≠a (2 a.m) | `cron(0 2 * * ? *)`       |

- **Tipo de Job:** Spark (Glue 3.0+)
- **TempDir:** Ruta S3 para datos temporales
- **IAM Role:** Permisos m√≠nimos para acceso a buckets y logs

---

## üìò Diccionario de Datos

| Campo           | Tabla               | Tipo       | Descripci√≥n                                                 |
|------------------|----------------------|------------|-------------------------------------------------------------|
| `event_id`       | `fact_user_events`   | string     | ID √∫nico del evento (hash `user_id` + `product_id` + `event_time`) |
| `event_time`     | `fact_user_events`   | timestamp  | Fecha y hora del evento                                     |
| `event_type`     | `fact_user_events`   | string     | Tipo de evento: `view`, `cart`, `purchase`                 |
| `user_id`        | Todas                | string     | Identificador √∫nico del usuario                             |
| `product_id`     | Todas                | string     | Identificador √∫nico del producto                            |
| `category_code`  | `dim_products`       | string     | Categor√≠a del producto (jerarqu√≠a tipo `electronics.smartphone`) |
| `brand`          | `dim_products`       | string     | Marca del producto                                          |
| `price`          | Todas                | float      | Precio en USD                                               |
| `user_session`   | `fact_user_events`   | string     | ID de sesi√≥n de navegaci√≥n del usuario                      |
| `hour_of_day`    | `fact_user_events`   | int        | Hora del evento (0 a 23)                                    |
| `day_of_week`    | `fact_user_events`   | int        | D√≠a de la semana (1=domingo, 7=s√°bado)                      |
| `event_date`     | Todas                | date       | Fecha del evento (para particionado)                        |


---
Muy bien, este **Paso 5** es clave para mostrar tu **capacidad de abstracci√≥n, dise√±o para escalabilidad** y pensamiento arquitect√≥nico. Ya tienes los cuatro escenarios que Nequi pide, y est√°n bien cubiertos. Ahora te propongo una **versi√≥n mejorada y m√°s desarrollada**, con una redacci√≥n m√°s t√©cnica, profesional y visualmente clara, que aporte contexto, decisiones justificadas y posibles tecnolog√≠as espec√≠ficas.

---

# üß© Paso 5: Redacci√≥n Final y Escenarios de Escalabilidad

## üß± ¬øPor qu√© se eligi√≥ esta arquitectura?

La arquitectura fue dise√±ada con base en principios de **simplicidad, escalabilidad y modularidad**:

- **Serverless & costo-efectiva:** Uso de Glue + Athena elimina la necesidad de administrar infraestructura.
- **Separaci√≥n de responsabilidades:** Modelo OLTP para captura operativa y Data Lake para an√°lisis.
- **Optimizaci√≥n por capas (raw-clean-model):** Mejora el control de calidad y permite trazabilidad.
- **Particionado y formato columnar (Parquet):** Aumenta el rendimiento anal√≠tico y reduce costos de lectura.
- **CDC con DMS:** Habilita replicaci√≥n continua desde Aurora sin afectar su rendimiento transaccional.

---

## üöÄ Escenarios de Escalabilidad y Arquitectura Alternativa

### üìà 1. Si los datos se incrementaran en 100x

> **Soluci√≥n:**  
> - Glue escalar√≠a horizontalmente con Spark, pero si los datos exceden esa capacidad, se puede migrar el procesamiento a **Amazon EMR** o **Databricks sobre AWS**.  
> - A nivel de consulta, **Redshift Spectrum** o **Athena con particionado fino** (`event_date`, `event_type`, `category`) permitir√≠an analizar eficientemente millones de registros por d√≠a.
> - Se puede habilitar **compresi√≥n ZSTD** y bucketing para mejorar rendimiento en S3.

---

### ‚è± 2. Si las tuber√≠as se ejecutaran diariamente en una ventana espec√≠fica

> **Soluci√≥n:**  
> - Se usar√≠an **Workflows y Triggers en AWS Glue** con dependencias entre jobs.  
> - **Ventanas programadas** v√≠a `cron` y alertas mediante **Amazon CloudWatch + SNS**.
> - Se integrar√≠a con **AWS Step Functions** para orquestaci√≥n visual y l√≥gica condicional (p.ej., solo ejecutar si hay nuevos datos en `raw/`).

---

### üë• 3. Si m√°s de 100 usuarios funcionales accedieran a los datos

> **Soluci√≥n:**  
> - Migraci√≥n de consultas a **Amazon Redshift** como almac√©n anal√≠tico compartido.  
> - Configuraci√≥n de **grupos de usuarios y roles con Amazon SSO / IAM** para control granular.  
> - Redshift permite cargas desde S3 (`COPY`) y consultas desde Spectrum si se quiere mantener el Data Lake como fuente principal.

---

### ‚ö° 4. Si se requiere anal√≠tica en tiempo real

> **Soluci√≥n:**  
> - Arquitectura pasar√≠a de batch a streaming:  
>   - **Amazon Kinesis Data Streams** o **MSK (Kafka)** para captura en tiempo real.  
>   - **Lambda + Firehose** para procesar y almacenar en **S3 (raw_stream/)** o escribir directo a Redshift Streaming.  
>   - Consultas en tiempo casi real con **Athena**, o incluso dashboards sobre **Amazon OpenSearch**.
> - Procesamiento complejo con **Apache Flink** sobre Kinesis para c√°lculos por ventana de tiempo o enriquecimiento de eventos.

---

## üß™ Reproducibilidad y escalabilidad

- Todos los scripts ETL est√°n modularizados, versionados y parametrizados por fecha.
- La infraestructura puede ser gestionada con Terraform o CloudFormation para despliegue automatizado.
- Pruebas unitarias garantizan confiabilidad ante cambios de l√≥gica o datos.

---

## ‚úÖ Conclusi√≥n

La soluci√≥n propuesta est√° pensada para un entorno empresarial real, aplicando principios de arquitectura moderna de datos:

- **Escalable**, **automatizable**, **analizable**.
- Listo para evolucionar a tiempo real o big data sin redise√±o desde cero.
- Compatible con herramientas modernas de BI, ciencia de datos y monitoreo.
- Adaptado al stack tecnol√≥gico de AWS con foco en performance y bajo costo.
