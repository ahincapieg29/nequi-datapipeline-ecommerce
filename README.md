# üõí eCommerce Data Pipeline

Dise√±o e implementaci√≥n de un pipeline moderno de datos en AWS para capturar, procesar y analizar eventos de comportamiento de usuarios en una tienda de comercio electr√≥nico.

## üß© Paso 1: Alcance del Proyecto y Captura de Datos

### üéØ Objetivo

Dise√±ar una arquitectura escalable de datos para una empresa de comercio electr√≥nico, enfocada en capturar, procesar y estructurar eventos de comportamiento de usuarios (como vistas de producto, adiciones al carrito y compras). El objetivo es habilitar flujos de valor anal√≠tico para **nutrir de datos a toda la compa√±√≠a**, soportando operaciones, Business Intelligence (BI) y Ciencia de Datos (DS).

Este ejercicio simula, a partir de un conjunto de datos de ejemplo descargado desde Kaggle, el dise√±o e implementaci√≥n de un pipeline de datos moderno: desde la ingesta hasta el modelado anal√≠tico, aplicando buenas pr√°cticas de calidad, gobierno y rendimiento sobre una arquitectura en AWS.

---

### üìÅ Dataset

- **Nombre:** eCommerce behavior data from multi category store  
- **Fuente:** [Kaggle - eCommerce behavior data](https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store) 
- **Archivo utilizado:** `2020-Apr.csv.gz`  
- **Tama√±o:** 66.589.268 registros  
- **Formato:** CSV comprimido (.gz)  
- **Frecuencia:** Abril 2020  

**Campos principales:**

- `event_time`: Fecha y hora del evento  
- `event_type`: Tipo de evento (`view`, `cart`, `remove_from_cart`, `purchase`)  
- `product_id`: ID del producto  
- `category_id`: ID de la categor√≠a  
- `category_code`: Categor√≠a espec√≠fica del producto  
- `brand`: Marca del producto  
- `price`: Precio del producto  
- `user_id`: ID an√≥nimo del usuario  
- `user_session`: ID de la sesi√≥n de navegaci√≥n  

üìå *Limitaciones conocidas:*  
El dataset solo representa un mes de eventos, y las sesiones de usuario son an√≥nimas. No incluye datos de usuarios autenticados ni eventos offline.

---

### üì¶ Captura de Datos

El archivo fue descargado desde Kaggle, descomprimido y ubicado en la ruta del proyecto:

```
/datos/raw/2020-Abr.csv.gz
```

Este ser√° utilizado como **fuente principal de datos** para construir el pipeline.

---

### üîç Casos de Uso Esperados

Este proyecto representa una soluci√≥n de datos para un **eCommerce** que busca consolidar una plataforma anal√≠tica empresarial capaz de alimentar diferentes √°reas:

#### üß† Para la organizaci√≥n

- Plataforma de datos unificada y gobernada  
- Integraci√≥n entre equipos de marketing, operaciones, BI y ciencia de datos  
- Toma de decisiones basada en datos reales del comportamiento de usuarios  

#### üß™ Business Intelligence (BI)

- Embudo de conversi√≥n: `view ‚Üí cart ‚Üí purchase`  
- Ranking de productos con mayor visualizaci√≥n vs. menor conversi√≥n  
- An√°lisis de sesiones por usuario y duraci√≥n promedio  
- Top categor√≠as y marcas por volumen de ventas  
- Evoluci√≥n de eventos por hora, d√≠a y semana  
- Comparativo de precios promedio por categor√≠a y marca  
- Identificaci√≥n de usuarios m√°s activos y recurrentes  

#### üî¨ Ciencia de Datos (Data Science)

- Segmentaci√≥n de usuarios basada en comportamiento  
- Predicci√≥n de probabilidad de compra  
- Detecci√≥n de anomal√≠as en precios o eventos  
- Clustering de productos por interacci√≥n y conversi√≥n  
- Modelos de propensi√≥n al abandono de carrito  
- Sistemas de recomendaci√≥n personalizados  

---

## üß© Paso 2: Exploraci√≥n y Evaluaci√≥n de Datos (EDA)

Para analizar un dataset de m√°s de **66 millones de registros**, se utiliz√≥ **PySpark** como motor de procesamiento distribuido. Gracias a su escalabilidad, se pudieron ejecutar transformaciones complejas y validaciones sin saturar el entorno de desarrollo.

Se tom√≥ una muestra aleatoria de aproximadamente **1.5 millones de registros** (~2.3% del total), lo que permiti√≥ realizar un **an√°lisis exploratorio eficiente** preservando la diversidad de tipos de eventos, productos y usuarios.

El an√°lisis exploratorio se realiz√≥ utilizando PySpark y se document√≥ en el notebook 

```
/notebooks/eda.ipynb.
```

---

### üîç Exploraci√≥n: Calidad de los Datos

#### üìå A. Valores Nulos Detectados

| Columna          | Valores Nulos |
|------------------|----------------|
| `brand`          | 205,961        |
| `category_code`  | 155,385        |
| `user_session`   | 2              |
| Resto de columnas| 0              |

#### üìå B. Registros Duplicados

- Total registros: **1,531,767**
- Registros √∫nicos: **1,531,619**
- **Duplicados detectados:** 148

#### üìå C. Valores √önicos por Columna

- `user_id`: 892,389
- `product_id`: 124,922
- `brand`: 3,661
- `category_code`: 139
- `event_time`: 1,085,863
- `user_session`: 1,281,641

#### üìå D. Tipos de Evento

| Tipo de evento | Registros |
|----------------|-----------|
| `view`         | 1,434,849 |
| `cart`         | 74,880    |
| `purchase`     | 22,038    |

‚Üí Representaci√≥n t√≠pica del embudo de conversi√≥n eCommerce: vistas > carritos > compras.

#### üìå E. Estad√≠sticas del Precio

| M√©trica   | Valor       |
|-----------|-------------|
| Count     | 1,531,767   |
| Media     | $273.11     |
| Desviaci√≥n est√°ndar | $356.13 |
| M√≠nimo    | $0.00       |
| M√°ximo    | $2,574.07   |

---

### üßº Sugerencias para la Limpieza de Datos

A partir de los hallazgos previos, se proponen las siguientes estrategias de limpieza para mejorar la calidad de los datos antes del modelado:

1. **Conversi√≥n de tipos**
   - Convertir `event_time` a `timestamp` con zona horaria UTC.
   - Tipificar `event_type` como variable categ√≥rica controlada (`view`, `cart`, `purchase`).

2. **Eliminaci√≥n de duplicados**
   - Remover registros completamente duplicados (id√©nticos en todas las columnas).

3. **Tratamiento de valores nulos**
   - Imputar `brand` y `category_code` con `"unknown"` cuando el porcentaje de nulos por categor√≠a sea bajo (<5%).
   - Omitir registros con `user_session` nulo (2 casos identificados).

4. **Filtrado de precios inv√°lidos**
   - Excluir registros con precio igual a 0 o negativo, ya que no representan comportamiento v√°lido.

5. **Verificaci√≥n de relaciones l√≥gicas**
   - Validar consistencia entre `user_id`, `user_session` y secuencia de `event_type`.
   - Confirmar flujos completos del embudo de conversi√≥n en sesiones (`view ‚Üí cart ‚Üí purchase`).

---

### üß™ Justificaci√≥n del Muestreo y Uso de PySpark

- üß† **Muestreo controlado (~1.5M filas)**: permite acelerar el desarrollo local sin sacrificar representatividad estad√≠stica.
- üî• **PySpark**: motor de procesamiento distribuido ideal para trabajar con datasets a gran escala como el original (66M+ registros), habilitando limpieza, transformaci√≥n y an√°lisis eficiente sobre AWS Glue u otros entornos.

---

## üß© Paso 3: Definici√≥n del Modelo de Datos y Arquitectura

### üß≠ Contexto

En un entorno de eCommerce moderno, los usuarios interact√∫an con una aplicaci√≥n m√≥vil generando millones de eventos mensuales (vistas de producto, adiciones al carrito, compras, etc.). Estos eventos son almacenados inicialmente en una **base de datos transaccional (OLTP)** como Aurora PostgreSQL, optimizada para escritura y consistencia. A partir de all√≠, se construye un pipeline para transformar y preparar los datos para uso anal√≠tico, dashboards de BI y ciencia de datos.

---

### üóÇÔ∏è Modelo de Datos Conceptual (OLTP)

#### Dise√±o inicial en Aurora PostgreSQL

La base de datos transaccional se dise√±√≥ utilizando un modelo **normalizado** con integridad referencial, ideal para registrar actividad desde la app m√≥vil en tiempo real.

**Entidades principales (PK y FK):**

- **USERS**  
  - `user_id` (Primary Key)  
  - `name`  
  - `email`  
  - `created_at`  

- **PRODUCTS**  
  - `product_id` (Primary Key)  
  - `name`  
  - `brand_id` (Foreign Key ‚Üí BRANDS.brand_id)  
  - `category_id` (Foreign Key ‚Üí CATEGORIES.category_id)  
  - `price`  
  - `stock`  

- **CATEGORIES**  
  - `category_id` (Primary Key)  
  - `category_name`  
  - `parent_id` (Foreign Key ‚Üí CATEGORIES.category_id)  

- **BRANDS**  
  - `brand_id` (Primary Key)  
  - `brand_name`  

- **SESSIONS**  
  - `session_id` (Primary Key)  
  - `user_id` (Foreign Key ‚Üí USERS.user_id)  
  - `device_type`  
  - `channel`  
  - `started_at`  

- **EVENTS**  
  - `event_id` (Primary Key)  
  - `session_id` (Foreign Key ‚Üí SESSIONS.session_id)  
  - `product_id` (Foreign Key ‚Üí PRODUCTS.product_id)  
  - `event_type` (ENUM: view, cart, purchase)  
  - `event_time`  
  - `price`  

**Ventajas del modelo OLTP:**

- Alta normalizaci√≥n garantiza consistencia y evita duplicaci√≥n  
- Relaciones referenciales para trazabilidad completa: usuario ‚Üí sesi√≥n ‚Üí evento  
- Optimizado para escritura intensiva  
- Preparado para replicaci√≥n CDC mediante AWS DMS hacia S3  

---

### üß± Modelo Anal√≠tico (Data Lake)

Una vez en S3, se aplica un proceso ETL para construir un modelo de datos orientado a an√°lisis.

#### Modelo en estrella optimizado para Athena / Redshift:

- **Tabla de hechos:** `fact_user_events`  
  - `event_id`, `event_time`, `event_type`  
  - `user_id`, `product_id`, `price`, `session_id`  
  - `category_name`, `brand` (denormalizados)  
  - `device_type`, `source_channel`, `day_of_week`, `hour_of_day`  

- **Dimensiones:**  
  - `dim_users`: Perfil de usuario  
  - `dim_products`: Productos  

**Capas del Data Lake en S3:**

- `raw/`: ingesti√≥n bruta desde DMS  
- `clean/`: datos validados, transformados  
- `model/`: modelo en estrella en formato Parquet  

---

### ‚öôÔ∏è Herramientas y Tecnolog√≠as Elegidas

| Componente                  | Tecnolog√≠a                         | Motivo de elecci√≥n                                                   |
|----------------------------|-------------------------------------|----------------------------------------------------------------------|
| Base de datos OLTP         | Aurora PostgreSQL                   | Escalable, transaccional, ideal para app m√≥vil                      |
| Replicaci√≥n continua       | AWS DMS (CDC)                       | Sincroniza datos sin afectar OLTP                                   |
| Almacenamiento             | Amazon S3                           | Econ√≥mico, escalable, nativo para Data Lake                         |
| Transformaci√≥n             | AWS Glue + PySpark                  | Procesamiento distribuido sobre alto volumen                        |
| Organizaci√≥n de datos      | Data Lake por capas (raw-clean-model)| Mejora trazabilidad, modularidad y control                          |
| Consulta anal√≠tica         | Athena                              | SQL serverless, bajo costo, ideal para exploraci√≥n y BI             |
| Visualizaci√≥n              | Amazon QuickSight, Power BI         | Integraci√≥n directa con Athena y Redshift                           |
| Formato de almacenamiento  | Parquet                             | Columnar, comprimido, altamente eficiente en an√°lisis               |

- AWS Glue fue elegido sobre Lambda + Step Functions porque el volumen de datos (66M+) y las transformaciones requeridas (join, filtrado, particionado) se benefician del procesamiento distribuido con PySpark.
- Aurora PostgreSQL permite escalabilidad transaccional con r√©plicas, ideal para integraci√≥n con CDC (Change Data Capture) usando AWS DMS.
- S3 es el almacenamiento √≥ptimo para un Data Lake escalable, y permite separaci√≥n por capas (`raw`, `clean`, `model`) con esquemas evolutivos.

---

### üîÅ Frecuencia de Actualizaci√≥n Recomendada

**Propuesta:** Actualizaci√≥n cada **1 hora** mediante **microlotes** para la tabla de hechos `fact_user_events`, y cargas **diarias** para dimensiones maestras (`dim_users`, `dim_products`).

**Justificaci√≥n:**

#### Para Business Intelligence (BI):
- Una actualizaci√≥n **cada hora** es suficiente para:
  - Monitorear comportamiento de usuarios en tiempo operativo
  - Medir rendimiento de campa√±as activas sin necesidad de real-time
  - Mantener dashboards √°giles con bajo costo computacional
  - Compatible con Power BI, QuickSight y Athena (consulta sobre particiones por fecha).

#### Para Ciencia de Datos (DS):
- Cargas **diarias** permiten:
  - Entrenamiento eficiente de modelos predictivos y an√°lisis exploratorio
  - Preparaci√≥n de features hist√≥ricas para clustering, scoring y segmentaci√≥n
  - Menor carga operativa y m√°s estabilidad en pipelines de entrenamiento

#### Capacidad t√©cnica:
- **AWS DMS** permite replicaci√≥n continua desde Aurora PostgreSQL hacia S3 (`raw/`).
- **AWS Glue** se puede ejecutar por cron cada hora para transformar solo los nuevos datos del d√≠a (`PROCESS_DATE=HOY`).
- El particionado por `event_date` permite cargas y consultas optimizadas en Athena y Redshift Spectrum.

---

### ‚úÖ Conclusi√≥n

Esta arquitectura permite:

- Separar las cargas OLTP de las anal√≠ticas, preservando rendimiento  
- Ingestar y transformar datos a gran escala sin impacto en producci√≥n  
- Ejecutar dashboards y modelos de an√°lisis con datos frescos y organizados  
- Evolucionar f√°cilmente hacia Redshift o Snowflake si la carga lo requiere  

La soluci√≥n cumple con las mejores pr√°cticas de AWS para arquitectura anal√≠tica moderna, aplicando herramientas serverless, formatos columnarizados, y un modelo escalable sin dependencias innecesarias.

---

# üß© Paso 4: Construcci√≥n del Pipeline ETL

Esta etapa implementa el procesamiento de datos desde una arquitectura OLTP en Aurora PostgreSQL (v√≠a CDC con AWS DMS) hasta un modelo anal√≠tico en S3 en formato Parquet, listo para consultas en Athena o visualizaciones en Power BI/QuickSight.

El pipeline procesa eventos de usuarios y actualiza dimensiones clave, garantizando consistencia, validaciones de calidad, y rendimiento.

---

## üèóÔ∏è Arquitectura T√©cnica

```
App m√≥vil ‚Üí Aurora PostgreSQL
              ‚Üì (CDC con AWS DMS)
       S3 (raw/)
              ‚Üì (PySpark en AWS Glue)
       S3 (model/fact_user_events, dim_*)
              ‚Üì
Athena / QuickSight / Power BI
```
---

## üîÅ Frecuencia de Procesamiento

| Tabla               | Frecuencia | Detalle                                            |
|---------------------|------------|----------------------------------------------------|
| `fact_user_events`  | Cada hora  | Microlote ‚Üí sobrescribe partici√≥n `event_date=HOY` |
| `dim_users`         | Diaria     | Carga completa desde eventos                       |
| `dim_products`      | Diaria     | Carga completa desde eventos                       |

---

## üóÇÔ∏è Estructura del Repositorio

```bash
/etl/
‚îú‚îÄ‚îÄ extract/
‚îÇ   ‚îî‚îÄ‚îÄ extract_from_s3.py              # Lectura de eventos del d√≠a
‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îú‚îÄ‚îÄ clean_and_transform_events.py   # Limpieza y validaci√≥n de eventos
‚îÇ   ‚îî‚îÄ‚îÄ transform_dimensions.py         # Carga diaria de usuarios y productos
‚îú‚îÄ‚îÄ load/
‚îÇ   ‚îî‚îÄ‚îÄ load_to_model.py                # Escritura en formato Parquet particionado
‚îú‚îÄ‚îÄ quality/
‚îÇ   ‚îî‚îÄ‚îÄ quality_checks.py               # Validaciones generales
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ spark_session.py                # Instancia de Spark para Glue/local
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ unit_tests_etl.py               # Pruebas unitarias
‚îî‚îÄ‚îÄ run_etl.py                          # Orquestador del proceso horario
```

---

## ‚úÖ Ejecuci√≥n ETL Horaria

### `run_etl.py`

```python
"""
Ejecuta la ETL cada hora:
1. Extrae eventos del d√≠a desde raw/
2. Limpia, transforma y valida
3. Carga partici√≥n del d√≠a a model/
4. Compara conteos raw vs model
"""

from extract.extract_from_s3 import extract_events
from transform.clean_and_transform_events import clean_transform
from load.load_to_model import load_events
from quality.quality_checks import compare_counts_between_layers
from utils.spark_session import get_spark_session
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    try:
        logger.info("üöÄ Iniciando ETL de eventos (horaria)")
        spark = get_spark_session("ETL-Hourly")
        df_raw = extract_events()
        df_transformed = clean_transform(df_raw)
        load_events(df_transformed)
        compare_counts_between_layers(spark)
        logger.info("‚úÖ ETL completada correctamente")
    except Exception as e:
        logger.error(f"‚ùå Error en la ETL: {e}")
        raise
```

---

### `extract/extract_from_s3.py`

```python
"""
Extrae eventos del d√≠a actual desde la capa raw/ en S3.
"""

from utils.spark_session import get_spark_session
from pyspark.sql.functions import current_date
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_events():
    spark = get_spark_session("ExtractEvents")
    logger.info("üì• Extrayendo eventos de S3/raw/")
    df = spark.read.parquet("s3://ecommerce-lake/raw/events/")
    return df.filter(df.event_date == current_date())
```

---

### `transform/clean_and_transform_events.py`

```python
"""
Limpia eventos, elimina duplicados, filtra precios, imputa datos y agrega campos temporales.
"""

from pyspark.sql.functions import col, hour, dayofweek
from quality.quality_checks import check_row_counts, check_nulls, check_uniqueness
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def clean_transform(df):
    logger.info("üßº Iniciando limpieza de eventos")
    df = df.dropDuplicates().filter(col("price") > 0)
    df = df.fillna({"brand": "unknown", "category_code": "unknown"})
    df = df.filter(col("user_session").isNotNull())
    df = df.withColumn("hour_of_day", hour("event_time")) \
           .withColumn("day_of_week", dayofweek("event_time"))

    check_row_counts(df, 10000)
    check_nulls(df, ["event_time", "event_type", "user_id", "product_id"])
    check_uniqueness(df, "event_id")
    return df
```

---

### `load/load_to_model.py`

```python
"""
Escribe eventos en model/fact_user_events/ particionando por event_date.
"""

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_events(df):
    path = "s3://ecommerce-lake/model/fact_user_events/"
    logger.info(f"üíæ Guardando eventos en {path}")
    df.write.mode("overwrite").partitionBy("event_date").parquet(path)
```

---

## üìö Carga de Dimensiones Diarias

### `transform/transform_dimensions.py`

```python
"""
Carga completa de dim_users y dim_products (una vez al d√≠a).
"""

from utils.spark_session import get_spark_session
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def transform_and_load_dimensions():
    spark = get_spark_session("ETL-Daily")

    # ----------- dim_users -----------
    logger.info("üë§ Procesando dim_users")
    df_users = spark.read.parquet("s3://ecommerce-lake/raw/events/") \
        .select("user_id").dropna().dropDuplicates()
    df_users.write.mode("overwrite").parquet("s3://ecommerce-lake/model/dim_users/")
    logger.info("‚úÖ dim_users cargada")

    # ----------- dim_products --------
    logger.info("üì¶ Procesando dim_products")
    df_products = spark.read.parquet("s3://ecommerce-lake/raw/events/") \
        .select("product_id", "brand", "category_code", "price") \
        .dropna(subset=["product_id", "price"]).dropDuplicates(["product_id"])
    df_products = df_products.fillna({"brand": "unknown", "category_code": "unknown"})
    df_products.write.mode("overwrite").parquet("s3://ecommerce-lake/model/dim_products/")
    logger.info("‚úÖ dim_products cargada")
```

---

## üîç Validaciones: `quality/quality_checks.py`

```python
"""
Valida conteos, nulos, unicidad y compara conteo entre capas.
"""

from pyspark.sql.functions import col, approx_count_distinct, current_date

def check_row_counts(df, min_expected):
    count = df.count()
    assert count >= min_expected, f"‚ùå Solo {count} registros, m√≠nimo requerido: {min_expected}"

def check_nulls(df, cols):
    for col in cols:
        nulls = df.filter(col(col).isNull()).count()
        assert nulls == 0, f"‚ùå Nulls encontrados en columna {col}: {nulls}"

def check_uniqueness(df, col_name):
    total = df.count()
    unique = df.select(approx_count_distinct(col_name)).collect()[0][0]
    assert unique == total, f"‚ùå Duplicados detectados en {col_name}"

def compare_counts_between_layers(spark):
    today = current_date()
    raw_count = spark.read.parquet("s3://ecommerce-lake/raw/events/") \
        .filter(col("event_date") == today).count()
    model_count = spark.read.parquet("s3://ecommerce-lake/model/fact_user_events/") \
        .filter(col("event_date") == today).count()
    assert model_count >= raw_count * 0.98, \
        f"‚ùå P√©rdida >2% entre RAW ({raw_count}) y MODEL ({model_count})"
```

---

## üß™ Tests: `tests/unit_tests_etl.py`

```python
"""
Pruebas autom√°ticas para las funciones de validaci√≥n de calidad.
"""

import unittest
from quality import quality_checks
from pyspark.sql import SparkSession

class TestETL(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.spark = SparkSession.builder.master("local[*]").appName("ETLTest").getOrCreate()

    def test_check_row_counts(self):
        df = self.spark.createDataFrame([(1,)], ["id"])
        quality_checks.check_row_counts(df, 1)

    def test_check_nulls(self):
        df = self.spark.createDataFrame([(1,), (2,)], ["id"])
        quality_checks.check_nulls(df, ["id"])

    def test_check_uniqueness(self):
        df = self.spark.createDataFrame([(1,), (1,)], ["event_id"])
        with self.assertRaises(AssertionError):
            quality_checks.check_uniqueness(df, "event_id")

if __name__ == "__main__":
    unittest.main()
```

---

## ‚òÅÔ∏è Ejecuci√≥n en AWS Glue

| Job                  | Script                      | Frecuencia       | Trigger Cron             |
|----------------------|-----------------------------|------------------|---------------------------|
| ETL Horaria Eventos  | `run_etl.py`                | Cada hora        | `cron(0 * ? * * *)`       |
| Carga Diaria Dim     | `transform_dimensions.py`   | Cada d√≠a (2 a.m) | `cron(0 2 * * ? *)`       |

- **Tipo de Job:** Spark
- **TempDir:** apuntar a un bucket S3
- **IAM Role:** con acceso a S3 de lectura y escritura

---

## üîÅ Reproducibilidad y Mantenibilidad

- **Particionado por `event_date`**
- **Logs estructurados** y trazables
- **C√≥digo versionado y testeado**
- **Par√°metros reutilizables**
- Compatible con **AWS Glue, Airflow, Step Functions**

---

## üìò Diccionario de Datos


---

## üß© Paso 5: Escenarios de Escalabilidad y Arquitectura Alternativa

- **üìà Si los datos crecieran 100x:**  
  Escalar√≠a Glue con Spark m√°s nodos, usaria Redshift Spectrum o EMR para anal√≠tica distribuida. Controlar√≠a particionamiento en S3 por `event_date`.

- **‚è± Si las tuber√≠as se ejecutaran diariamente en una ventana de tiempo espec√≠fica:**  
  Usar√≠a AWS Glue triggers + workflows + monitoreo con CloudWatch y alertas por SNS.

- **üë• Si m√°s de 100 usuarios funcionales accedieran a la BD:**  
  Implementar√≠a Redshift + Amazon SSO + rol de acceso y pol√≠ticas IAM controladas por recurso.

- **‚ö° Si se requiere anal√≠tica en tiempo real:**  
  Cambiar√≠a de arquitectura batch a **Kinesis Data Streams** + **Lambda + Firehose** + **Athena o Redshift Streaming**.
